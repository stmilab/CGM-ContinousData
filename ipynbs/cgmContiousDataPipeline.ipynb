{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, pdb, copy\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_time_series_data(df, cgm_cols=[\"Dexcom GL\", \"Libre GL\"], activity_cols=[\"HR\", \"METs\"]):\n",
    "    \"\"\"Clean and validate time-series data\"\"\"\n",
    "    # Copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Handle missing values\n",
    "    for col in cgm_cols + activity_cols:\n",
    "        if col in df.columns:\n",
    "            # Linear interpolation with 5-minute window\n",
    "            df[col] = df[col].interpolate(method='linear', limit=5)\n",
    "            # Forward/backward fill remaining\n",
    "            df[col] = df[col].ffill().bfill()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_daily_traces(\n",
    "    dataset_df: pd.DataFrame, \n",
    "    subject_id: int,\n",
    "    cgm_cols=[\"Dexcom GL\", \"Libre GL\"],\n",
    "    activity_cols=[\"HR\", \"METs\"],\n",
    "    img_size=(112, 112),\n",
    "    start_hour=4  # Start at 6 AM\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced version with full-day (1440 minute) time features.\n",
    "    \"\"\"\n",
    "    # Cleaning functions remain the same\n",
    "    def clean_series(series):\n",
    "        series = series.interpolate(method='linear', limit=5).ffill().bfill()\n",
    "        try:\n",
    "            from scipy.signal import savgol_filter\n",
    "            series = savgol_filter(series, window_length=15, polyorder=2)\n",
    "        except ImportError:\n",
    "            series = series.rolling(window=15, min_periods=1, center=True).mean()\n",
    "        return series\n",
    "\n",
    "    # Apply cleaning\n",
    "    cleaned_df = dataset_df.copy()\n",
    "    for col in cgm_cols + activity_cols:\n",
    "        if col in cleaned_df.columns:\n",
    "            cleaned_df[col] = clean_series(cleaned_df[col])\n",
    "    \n",
    "    # Resample to 1-minute frequency\n",
    "    resampled_df = cleaned_df.resample('1min').ffill(limit=5)\n",
    "    \n",
    "    # Fill NaNs with median\n",
    "    for col in cgm_cols + activity_cols:\n",
    "        if col in resampled_df.columns:\n",
    "            resampled_df[col] = resampled_df[col].fillna(resampled_df[col].median())\n",
    "    \n",
    "    # Statistics for normalization\n",
    "    cgm_stats = {\n",
    "        'mean': resampled_df[cgm_cols].mean().values if all(col in resampled_df.columns for col in cgm_cols) else np.zeros(len(cgm_cols)),\n",
    "        'std': resampled_df[cgm_cols].std().values if all(col in resampled_df.columns for col in cgm_cols) else np.ones(len(cgm_cols))\n",
    "    }\n",
    "    activity_stats = {\n",
    "        'mean': resampled_df[activity_cols].mean().values if all(col in resampled_df.columns for col in activity_cols) else np.zeros(len(activity_cols)),\n",
    "        'std': resampled_df[activity_cols].std().values if all(col in resampled_df.columns for col in activity_cols) else np.ones(len(activity_cols))\n",
    "    }\n",
    "\n",
    "    # Initialize arrays and dictionaries\n",
    "    days = pd.Series(resampled_df.index.date).unique()\n",
    "    days_list = [str(day) for day in days]\n",
    "    cgm_daily_data = np.full((len(days), len(cgm_cols), 1440), np.nan)\n",
    "    activity_daily_data = np.full((len(days), len(activity_cols), 1440), np.nan)\n",
    "    image_data_by_day = {}\n",
    "    nutrition_data_by_day = {}\n",
    "    timestamp_vectors = {}  # Store full day timestamps\n",
    "    meal_timing_features = {}  # Store meal timing features for full day\n",
    "    \n",
    "    # Time window parameters\n",
    "    minutes_after_last_meal = 6 * 60  # 6 hours after last meal\n",
    "\n",
    "    # Process each day\n",
    "    for i, day in enumerate(days):\n",
    "        day_start = pd.Timestamp(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1) - pd.Timedelta(minutes=1)\n",
    "        day_data = resampled_df.loc[day_start:day_end]\n",
    "        \n",
    "        # Extract meal information for this day\n",
    "        meal_rows = dataset_df.loc[day_start:day_end].dropna(subset=['Meal Type'])\n",
    "        meal_times = meal_rows.index.sort_values() if not meal_rows.empty else []\n",
    "        \n",
    "        # Create full day timestamp vector (1440 minutes)\n",
    "        full_day_timestamps = [day_start + pd.Timedelta(minutes=m) for m in range(1440)]\n",
    "        timestamp_vectors[str(day)] = full_day_timestamps\n",
    "        \n",
    "        # Initialize full day meal timing features\n",
    "        meal_timing = np.zeros((5, 1440))  # 5 features for 1440 minutes\n",
    "        \n",
    "        # Set default values for all minutes\n",
    "        meal_timing[0, :] = -1  # Minutes since most recent meal\n",
    "        meal_timing[1, :] = -1  # Minutes until next meal\n",
    "        meal_timing[2, :] = 0   # Is within 2 hours after meal\n",
    "        meal_timing[3, :] = 0   # Count of previous meals\n",
    "        meal_timing[4, :] = np.arange(1440)  # Minutes since start of day\n",
    "        \n",
    "        # Determine time window for processing (we'll still store full day)\n",
    "        if len(meal_times) == 0:\n",
    "            # If no meals, use default window (6 AM to midnight)\n",
    "            window_start = day_start + pd.Timedelta(hours=start_hour)\n",
    "            window_end = day_end\n",
    "        else:\n",
    "            # Start at 6 AM\n",
    "            window_start = day_start + pd.Timedelta(hours=start_hour)\n",
    "            \n",
    "            # End 6 hours after the last meal or at day end, whichever is earlier\n",
    "            last_meal_time = meal_times[-1]\n",
    "            last_meal_plus_6h = last_meal_time + pd.Timedelta(minutes=minutes_after_last_meal)\n",
    "            window_end = min(last_meal_plus_6h, day_end)\n",
    "        \n",
    "        # Filter data to our window\n",
    "        window_data = day_data.loc[window_start:window_end]\n",
    "        \n",
    "        # Calculate window minutes for validation/debugging\n",
    "        window_minutes = len(window_data)\n",
    "        \n",
    "        if not window_data.empty:\n",
    "            # Store CGM and activity data for the window\n",
    "            for j, col in enumerate(cgm_cols):\n",
    "                if col in window_data.columns:\n",
    "                    # Get minutes of day for each window timestamp\n",
    "                    minutes_of_day = (window_data.index.hour * 60 + window_data.index.minute).values\n",
    "                    \n",
    "                    vals = window_data[col].values\n",
    "                    vals = np.nan_to_num(vals, nan=window_data[col].median())\n",
    "                    cgm_daily_data[i, j, minutes_of_day] = vals\n",
    "            \n",
    "            for j, col in enumerate(activity_cols):\n",
    "                if col in window_data.columns:\n",
    "                    # Get minutes of day for each window timestamp\n",
    "                    minutes_of_day = (window_data.index.hour * 60 + window_data.index.minute).values\n",
    "                    \n",
    "                    vals = window_data[col].values\n",
    "                    vals = np.nan_to_num(vals, nan=window_data[col].median())\n",
    "                    activity_daily_data[i, j, minutes_of_day] = vals\n",
    "            \n",
    "            # Calculate meal timing features for each timestamp in the full day\n",
    "            for minute in range(1440):\n",
    "                timestamp = day_start + pd.Timedelta(minutes=minute)\n",
    "                \n",
    "                # 1. Minutes since most recent meal\n",
    "                prev_meals = [m for m in meal_times if m <= timestamp]\n",
    "                if prev_meals:\n",
    "                    meal_timing[0, minute] = (timestamp - prev_meals[-1]).total_seconds()/60\n",
    "                # else keep default -1\n",
    "                \n",
    "                # 2. Minutes until next meal\n",
    "                next_meals = [m for m in meal_times if m > timestamp]\n",
    "                if next_meals:\n",
    "                    meal_timing[1, minute] = (next_meals[0] - timestamp).total_seconds()/60\n",
    "                # else keep default -1\n",
    "                \n",
    "                # 3. Boolean: Is this within 2 hours after a meal?\n",
    "                meal_timing[2, minute] = 1 if (meal_timing[0, minute] >= 0 and meal_timing[0, minute] <= 120) else 0\n",
    "                \n",
    "                # 4. Count of previous meals for the day\n",
    "                meal_timing[3, minute] = len(prev_meals)\n",
    "                \n",
    "                # 5. Minutes since start of day is already set to minute number\n",
    "        \n",
    "        # Store meal timing features (full day)\n",
    "        meal_timing_features[str(day)] = meal_timing\n",
    "        \n",
    "        # Process nutrition data with more detailed information\n",
    "        day_str = str(day)\n",
    "        original_day_data = dataset_df.loc[day_start:day_end]\n",
    "        \n",
    "        nutrition_rows = original_day_data.dropna(subset=['Calories', 'Carbs', 'Protein', 'Fat', 'Fiber'], how='all')\n",
    "        day_nutrition = []\n",
    "        \n",
    "        # Enhanced meal information\n",
    "        meal_counter = {}  # Track meal numbers by type\n",
    "        \n",
    "        # Process meals in chronological order\n",
    "        for ts, row in nutrition_rows.iterrows():\n",
    "            meal_type = row['Meal Type'] if pd.notna(row['Meal Type']) else 'Unknown'\n",
    "            \n",
    "            # Increment meal counter for this type\n",
    "            if meal_type not in meal_counter:\n",
    "                meal_counter[meal_type] = 1\n",
    "            else:\n",
    "                meal_counter[meal_type] += 1\n",
    "                \n",
    "            # Calculate meal timing within day\n",
    "            minutes_since_day_start = (ts - day_start).total_seconds() / 60\n",
    "            hour_of_day = ts.hour + ts.minute/60\n",
    "            \n",
    "            nutrition = {\n",
    "                'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'MealType': meal_type,\n",
    "                'MealNumber': meal_counter[meal_type],  # Which breakfast/lunch/dinner/snack is this?\n",
    "                'MinuteOfDay': int(minutes_since_day_start),\n",
    "                'HourOfDay': hour_of_day,\n",
    "                'calories': row['Calories'] if pd.notna(row['Calories']) else 0,\n",
    "                'carbs': row['Carbs'] if pd.notna(row['Carbs']) else 0,\n",
    "                'protein': row['Protein'] if pd.notna(row['Protein']) else 0,\n",
    "                'fat': row['Fat'] if pd.notna(row['Fat']) else 0,\n",
    "                'fiber': row['Fiber'] if pd.notna(row['Fiber']) else 0,\n",
    "                'has_image': pd.notna(row['Image path'])\n",
    "            }\n",
    "            day_nutrition.append(nutrition)\n",
    "        \n",
    "        # Add meal sequence information\n",
    "        if day_nutrition:\n",
    "            # Sort by timestamp\n",
    "            day_nutrition = sorted(day_nutrition, key=lambda x: x['MinuteOfDay'])\n",
    "            \n",
    "            # Add meal sequence number and intervals\n",
    "            for k in range(len(day_nutrition)):\n",
    "                day_nutrition[k]['MealSequence'] = k + 1  # 1-based meal sequence for the day\n",
    "                \n",
    "                # Time to next meal\n",
    "                if k < len(day_nutrition) - 1:\n",
    "                    day_nutrition[k]['MinutesToNextMeal'] = day_nutrition[k+1]['MinuteOfDay'] - day_nutrition[k]['MinuteOfDay']\n",
    "                else:\n",
    "                    day_nutrition[k]['MinutesToNextMeal'] = -1  # No next meal\n",
    "                \n",
    "                # Time since previous meal\n",
    "                if k > 0:\n",
    "                    day_nutrition[k]['MinutesSincePrevMeal'] = day_nutrition[k]['MinuteOfDay'] - day_nutrition[k-1]['MinuteOfDay']\n",
    "                else:\n",
    "                    day_nutrition[k]['MinutesSincePrevMeal'] = -1  # No previous meal\n",
    "        \n",
    "        nutrition_data_by_day[day_str] = day_nutrition\n",
    "        \n",
    "        # Image data processing remains largely the same\n",
    "        image_rows = original_day_data.dropna(subset=['Image path'])\n",
    "        day_images = []\n",
    "        for ts, row in image_rows.iterrows():\n",
    "            try:\n",
    "                img_data = get_image(row['Image path'], subject_id, img_size)\n",
    "                # Calculate timing features for this image/meal\n",
    "                minutes_since_day_start = (ts - day_start).total_seconds() / 60\n",
    "                \n",
    "                metadata = {\n",
    "                    'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'minute_of_day': int(minutes_since_day_start),\n",
    "                    'meal_type': row['Meal Type'] if 'Meal Type' in row else None,\n",
    "                    'calories': row['Calories'] if 'Calories' in row else None,\n",
    "                    'carbs': row['Carbs'] if 'Carbs' in row else None,\n",
    "                    'protein': row['Protein'] if 'Protein' in row else None,\n",
    "                    'fat': row['Fat'] if 'Fat' in row else None,\n",
    "                    'fiber': row['Fiber'] if 'Fiber' in row else None\n",
    "                }\n",
    "                day_images.append({'image': img_data, 'metadata': metadata})\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        image_data_by_day[day_str] = day_images if day_images else []\n",
    "\n",
    "    # Window metadata\n",
    "    window_metadata = {\n",
    "        'start_hour': start_hour,\n",
    "        'hours_after_last_meal': minutes_after_last_meal / 60,\n",
    "        'full_day_length': 1440\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        days_list,\n",
    "        cgm_daily_data,\n",
    "        activity_daily_data,\n",
    "        image_data_by_day,\n",
    "        nutrition_data_by_day,\n",
    "        cgm_stats,\n",
    "        activity_stats,\n",
    "        window_metadata,\n",
    "        timestamp_vectors,        # Now: full day timestamps\n",
    "        meal_timing_features      # Now: full day meal timing features\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_CGMacros(\n",
    "    subject_id: int,\n",
    "    csv_dir: str = \"CGMacros-2\",\n",
    ") -> pd.DataFrame:\n",
    "    if type(subject_id) != int:\n",
    "        print(\"subject_id should be an integer\")\n",
    "        raise ValueError\n",
    "    subejct_path = f\"CGMacros-{subject_id:03d}/CGMacros-{subject_id:03d}.csv\"\n",
    "    subject_file = os.path.join(csv_dir, subejct_path)\n",
    "    if not os.path.exists(subject_file):\n",
    "        tqdm.write(f\"File {subject_file} not found\")\n",
    "        raise FileNotFoundError\n",
    "    dataset_df = pd.read_csv(subject_file, index_col=None)\n",
    "    dataset_df[\"Timestamp\"] = pd.to_datetime(dataset_df[\"Timestamp\"])\n",
    "    dataset_df = clean_time_series_data(dataset_df)  # Add cleaning step\n",
    "    return dataset_df.set_index(\"Timestamp\")\n",
    "\n",
    "def get_image(\n",
    "    img_filename: str,\n",
    "    subject_id: int,\n",
    "    target_size: tuple,\n",
    "    cgmacros_path: str = \"CGMacros-2/\",\n",
    ") -> np.ndarray:\n",
    "    subject_path = f\"CGMacros-{subject_id:03d}/\"\n",
    "    img_path = f\"{cgmacros_path}{subject_path}{img_filename}\"\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"File {img_path} not found\")\n",
    "        raise FileNotFoundError\n",
    "    # Loading names out\n",
    "    img_data = cv2.resize(\n",
    "        cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB),\n",
    "        target_size,\n",
    "        interpolation=cv2.INTER_LANCZOS4,\n",
    "    )\n",
    "    return img_data\n",
    "def create_daily_dataset(\n",
    "    subject_id: int,\n",
    "    csv_dir: str = \"CGMacros 2\",\n",
    "    cgm_cols=[\"Dexcom GL\", \"Libre GL\"],\n",
    "    activity_cols=[\"HR\",\"METs\"],\n",
    "    img_size=(112, 112),\n",
    "    start_hour=6,\n",
    "    verbose=False\n",
    "):\n",
    "    try:\n",
    "        # Load data with column validation\n",
    "        dataset_df = load_CGMacros(subject_id, csv_dir)\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Available columns:\", dataset_df.columns.tolist())\n",
    "        \n",
    "        # Handle missing columns gracefully\n",
    "        available_activity_cols = [col for col in activity_cols \n",
    "                                 if col in dataset_df.columns]\n",
    "        if len(available_activity_cols) < len(activity_cols):\n",
    "            print(f\"Warning: Missing activity columns. Using {available_activity_cols} for subject {subject_id}\")\n",
    "        \n",
    "        # Process data with validated columns and custom time window\n",
    "        result = load_daily_traces(\n",
    "            dataset_df, subject_id, \n",
    "            cgm_cols=cgm_cols,\n",
    "            activity_cols=available_activity_cols,\n",
    "            img_size=img_size,\n",
    "            start_hour=start_hour\n",
    "        )\n",
    "        \n",
    "        return (subject_id,) + result  # Return all elements with subject_id\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Data for subject {subject_id} not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject {subject_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_subjects(\n",
    "    subject_ids=None,\n",
    "    csv_dir=\"CGMacros-2\",\n",
    "    demographics_path = \"demographicPCA.csv\",\n",
    "    save_dir=\"processed_data/\",\n",
    "    cgm_cols=[\"Dexcom GL\",\"Libre GL\"],\n",
    "    activity_cols=[\"HR\",\"METs\"],\n",
    "    img_size=(112, 112),\n",
    "    start_hour=6\n",
    "):\n",
    "    # Create save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    if subject_ids is None:\n",
    "        subject_ids = range(1, 51)  # Try subjects 1-50\n",
    "    \n",
    "    summary = {\n",
    "        'processed_subjects': [],\n",
    "        'total_days': 0,\n",
    "        'total_images': 0,\n",
    "        'total_meals': 0\n",
    "    }\n",
    "    \n",
    "    for subject_id in tqdm(subject_ids, desc=\"Processing subjects\"):\n",
    "        result = create_daily_dataset(subject_id, csv_dir, start_hour=start_hour)\n",
    "        if result is None:\n",
    "            continue\n",
    "            \n",
    "        # Unpack all return values\n",
    "        (subject_id, days, cgm, activity, images, nutrition, \n",
    "         cgm_stats, activity_stats, window_metadata, \n",
    "         timestamp_vectors, meal_timing_features) = result\n",
    "        \n",
    "        pca_df = pd.read_csv(demographics_path)\n",
    "        pca_vector = pca_df[pca_df['SubjectID'] == subject_id].iloc[0, 1:].values.astype(\"float32\")\n",
    "\n",
    "        \n",
    "        # Save data with all new features\n",
    "        subject_data = {\n",
    "            'subject_id': subject_id,\n",
    "            'days': days,\n",
    "            'cgm_data': cgm,\n",
    "            'activity_data': activity,\n",
    "            'image_data': images,\n",
    "            'nutrition_data': nutrition,\n",
    "            'cgm_stats': cgm_stats,\n",
    "            'activity_stats': activity_stats,\n",
    "            'demographics':pca_vector,\n",
    "            'window_metadata': window_metadata,\n",
    "            'timestamp_vectors': timestamp_vectors,  # New: store timestamps\n",
    "            'meal_timing_features': meal_timing_features  # New: meal timing features\n",
    "        }\n",
    "        torch.save(subject_data, os.path.join(save_dir, f\"subject_{subject_id:03d}_daily_data.pt\"))\n",
    "        \n",
    "        # Update summary counts\n",
    "        summary['processed_subjects'].append(subject_id)\n",
    "        summary['total_days'] += len(days)\n",
    "        summary['total_images'] += sum(len(imgs) for imgs in images.values())\n",
    "        summary['total_meals'] += sum(len(meals) for meals in nutrition.values())\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DailyTracesDataset(Dataset):\n",
    "    def __init__(self, data_dir, subject_ids=None, transform=None, skip_days=[1]):\n",
    "        # Initialization remains the same\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.skip_days = skip_days or [1]\n",
    "        transform = None\n",
    "        \n",
    "        # Find relevant subject files\n",
    "        if subject_ids is None:\n",
    "            self.data_files = [\n",
    "                f for f in os.listdir(data_dir) \n",
    "                if f.startswith(\"subject_\") and f.endswith(\"_daily_data.pt\")\n",
    "            ]\n",
    "        else:\n",
    "            self.data_files = [\n",
    "                f\"subject_{sid:03d}_daily_data.pt\" \n",
    "                for sid in subject_ids \n",
    "                if os.path.exists(os.path.join(data_dir, f\"subject_{sid:03d}_daily_data.pt\"))\n",
    "            ]\n",
    "        \n",
    "        # Build indices accounting for skip_days\n",
    "        self.indices = []\n",
    "        self.subject_day_pairs = []\n",
    "        \n",
    "        for file_idx, fname in enumerate(self.data_files):\n",
    "            data = torch.load(os.path.join(data_dir, fname), weights_only=False)\n",
    "            subject_id = data['subject_id']\n",
    "            \n",
    "            for day_idx, day_str in enumerate(data['days']):\n",
    "                day_num = int(day_str.split('-')[2])\n",
    "                if day_num not in self.skip_days:\n",
    "                    self.indices.append((file_idx, day_idx))\n",
    "                    self.subject_day_pairs.append((subject_id, day_num))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, day_idx = self.indices[idx]\n",
    "        data = torch.load(os.path.join(self.data_dir, self.data_files[file_idx]), weights_only=False)\n",
    "        day = data['days'][day_idx]\n",
    "        \n",
    "        # Apply transforms if specified\n",
    "        def _apply_transform(x):\n",
    "            return self.transform(x) if self.transform else x\n",
    "        \n",
    "        demographics_path = 'demographicPCA.csv'\n",
    "        pca_df = pd.read_csv(demographics_path)\n",
    "        subject_data = pca_df[pca_df['SubjectID'] == data['subject_id']]\n",
    "        pca_values = subject_data[['PCA_1', 'PCA_2', 'PCA_3', 'PCA_4', 'PCA_5']].values\n",
    "\n",
    "        # Include the new features in the returned item\n",
    "        return {\n",
    "            'subject_id': data['subject_id'],\n",
    "            'day': day,\n",
    "            'cgm_data': _apply_transform(data['cgm_data'][day_idx]),\n",
    "           'demographics':  torch.tensor(pca_values.flatten(), dtype=torch.float32),\n",
    "            'activity_data': _apply_transform(data['activity_data'][day_idx]),\n",
    "            'images': [_apply_transform(img['image']) for img in data['image_data'].get(day, [])],\n",
    "            'nutrition': data['nutrition_data'].get(day, []),\n",
    "            'subject_day_pair': self.subject_day_pairs[idx],\n",
    "            'timestamps': data.get('timestamp_vectors', {}).get(day, []),  # New: timestamps\n",
    "            'meal_timing_features': data.get('meal_timing_features', {}).get(day, [])  # New: meal timing features\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    \"\"\"Handles variable-length nutrition data, images and timestamp vectors\"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def fix_nans(array):\n",
    "        \"\"\"Replace NaNs with median (per channel)\"\"\"\n",
    "        median_vals = np.nanmedian(array, axis=1, keepdims=True)\n",
    "        return np.where(np.isnan(array), median_vals, array)\n",
    "\n",
    "    # Fix NaNs before converting to tensors\n",
    "    for i, x in enumerate(batch):\n",
    "        x['cgm_data'] = fix_nans(x['cgm_data'])\n",
    "        x['activity_data'] = fix_nans(x['activity_data'])\n",
    "        if 'meal_timing_features' in x and len(x['meal_timing_features']) > 0:\n",
    "            x['meal_timing_features'] = fix_nans(x['meal_timing_features'])\n",
    "\n",
    "    return {\n",
    "        'subject_ids': torch.tensor([x['subject_id'] for x in batch]),\n",
    "        'days': [x['day'] for x in batch],\n",
    "        \n",
    "        'cgm_data': torch.stack([torch.tensor(x['cgm_data'], dtype=torch.float32) for x in batch]),\n",
    "        'activity_data': torch.stack([torch.tensor(x['activity_data'], dtype=torch.float32) for x in batch]),\n",
    "        'images': [x['images'] for x in batch],  # List of lists\n",
    "        'nutrition': [x['nutrition'] for x in batch],  # List of lists\n",
    "        'subject_day_pairs': [x['subject_day_pair'] for x in batch],\n",
    "        'timestamps': [x.get('timestamps', []) for x in batch],  # New: timestamps for each data point\n",
    "        'meal_timing_features': [torch.tensor(x.get('meal_timing_features', np.zeros((5, 1))), \n",
    "                                              dtype=torch.float32) if len(x.get('meal_timing_features', [])) > 0 \n",
    "                                 else torch.zeros((5, 1)) for x in batch],  # New: meal timing features,\n",
    "        'demographics': torch.stack([x['demographics'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_by_subject_day(dataset, test_size=0.2, random_state=2025):\n",
    "    \"\"\"\n",
    "    Split the dataset based on subject-day pairs to ensure all data from\n",
    "    the same subject and day stays together in either training or testing set.\n",
    "    \n",
    "    Args:\n",
    "        dataset (DailyTracesDataset): The dataset to split\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_indices, test_indices)\n",
    "    \"\"\"\n",
    "    # Get unique subject-day pairs\n",
    "    subject_day_df = pd.DataFrame(dataset.subject_day_pairs, columns=['subject_id', 'day_id'])\n",
    "    unique_pairs = subject_day_df.drop_duplicates()\n",
    "    \n",
    "    # Split the unique subject-day pairs\n",
    "    train_pairs, test_pairs = train_test_split(\n",
    "        unique_pairs, \n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Convert to sets for faster lookup\n",
    "    train_pairs_set = set(zip(train_pairs['subject_id'], train_pairs['day_id']))\n",
    "    test_pairs_set = set(zip(test_pairs['subject_id'], test_pairs['day_id']))\n",
    "    \n",
    "    # Create masks for train and test indices\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    for i, (subject_id, day_id) in enumerate(dataset.subject_day_pairs):\n",
    "        if (subject_id, day_id) in train_pairs_set:\n",
    "            train_indices.append(i)\n",
    "        elif (subject_id, day_id) in test_pairs_set:\n",
    "            test_indices.append(i)\n",
    "    \n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "class SubjectDaySubset(Dataset):\n",
    "    \"\"\"\n",
    "    Subset of DailyTracesDataset based on indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "def get_train_test_datasets(data_dir, subject_ids=None, test_size=0.2, random_state=2025, transform=None):\n",
    "    \"\"\"\n",
    "    Get train and test datasets split by subject-day pairs.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Directory containing processed data\n",
    "        subject_ids (list): List of subject IDs to include. If None, include all available.\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        random_state (int): Random seed for reproducibility\n",
    "        transform (callable): Optional transform to apply to the data\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_dataset, test_dataset)\n",
    "    \"\"\"\n",
    "    # Create the full dataset\n",
    "    full_dataset = DailyTracesDataset(data_dir, subject_ids, transform)\n",
    "    \n",
    "    # Split by subject-day pairs\n",
    "    train_indices, test_indices = split_dataset_by_subject_day(full_dataset, test_size, random_state)\n",
    "    \n",
    "    # Create train and test subsets\n",
    "    train_dataset = SubjectDaySubset(full_dataset, train_indices)\n",
    "    test_dataset = SubjectDaySubset(full_dataset, test_indices)\n",
    "    \n",
    "    print(f\"Full dataset size: {len(full_dataset)}\")\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_samples_by_nutrition(dataset):\n",
    "    \"\"\"\n",
    "    Filters samples that don't meet nutrition quality requirements:\n",
    "    - At least 2 distinct meals out of breakfast, lunch, dinner\n",
    "    - Non-zero total calories\n",
    "    - Total calories >= 300\n",
    "    \"\"\"\n",
    "    valid_meals = {\"breakfast\", \"lunch\", \"dinner\"}\n",
    "    filtered = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        nutrition = sample.get(\"nutrition\", [])\n",
    "        if not nutrition or not isinstance(nutrition, list):\n",
    "            continue\n",
    "\n",
    "        meal_types_present = {meal.get(\"MealType\", \"\").lower() for meal in nutrition}\n",
    "        relevant_meals = meal_types_present & valid_meals\n",
    "\n",
    "        total_calories = sum(meal.get(\"calories\", 0) for meal in nutrition)\n",
    "        calories_are_valid = total_calories > 0 and total_calories >= 300\n",
    "\n",
    "        if len(relevant_meals) >= 2 and calories_are_valid:\n",
    "            filtered.append(sample)\n",
    "\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def main():\n",
    "    # Step 1: Process the raw CSV data for subjects and save the daily traces\n",
    "    # You can adjust the subject IDs, directories, etc. as needed\n",
    "    summary = process_multiple_subjects(\n",
    "        subject_ids=range(1, 50),  # Process subjects 1-49\n",
    "        csv_dir=\"CGMacros-2\",  # Path to your CSV files\n",
    "        save_dir=\"processed_data/\",  # Where to save processed data\n",
    "        cgm_cols=[\"Dexcom GL\", \"Libre GL\"],\n",
    "        activity_cols=[\"HR\",\"METs\"],\n",
    "        img_size=(112, 112),\n",
    "        start_hour=6  # New parameter: starting hour (6 AM)\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing summary:\")\n",
    "    print(f\"- Processed {len(summary['processed_subjects'])} subjects\")\n",
    "    print(f\"- Total days: {summary['total_days']}\")\n",
    "    print(f\"- Total images: {summary['total_images']}\")\n",
    "    print(f\"- Total meals: {summary['total_meals']}\")  # Added meal count\n",
    "    \n",
    "    # Step 2: Create train and test datasets from the processed data\n",
    "    train_dataset, test_dataset = get_train_test_datasets(\n",
    "        data_dir=\"processed_data/\",\n",
    "        subject_ids=None,  # Use all available subjects\n",
    "        test_size=0.2,  # 80% train, 20% test\n",
    "        random_state=2025,  # For reproducibility\n",
    "        transform=None  # Add any transforms you need\n",
    "    )\n",
    "    \n",
    "    print(\"Done creating train and test datasets\")\n",
    "    \n",
    "    \n",
    "\n",
    "    #Step 3: Create DataLoaders for efficient batching\n",
    "    train_loader = DataLoader(\n",
    "        filter_samples_by_nutrition(train_dataset),\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=custom_collate\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        filter_samples_by_nutrition(test_dataset),\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=custom_collate\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:   0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros-2/CGMacros-001/photos/00000075-PHOTO-2019-4-5-9-48-0.jpg not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:   2%|▏         | 1/49 [00:03<02:59,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros-2/CGMacros-002/photos/00000082-PHOTO-2021-1-12-11-37-0.jpg not found\n",
      "File CGMacros-2/CGMacros-002/photos/00000083-PHOTO-2021-1-12-12-13-0.jpg not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:  47%|████▋     | 23/49 [01:05<01:20,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros-2/CGMacros-024/CGMacros-024.csv not found\n",
      "Data for subject 24 not found.\n",
      "File CGMacros-2/CGMacros-025/CGMacros-025.csv not found\n",
      "Data for subject 25 not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:  73%|███████▎  | 36/49 [01:32<00:32,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros-2/CGMacros-037/CGMacros-037.csv not found\n",
      "Data for subject 37 not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:  80%|███████▉  | 39/49 [01:37<00:20,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros-2/CGMacros-040/CGMacros-040.csv not found\n",
      "Data for subject 40 not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects: 100%|██████████| 49/49 [01:57<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing subject 49: cannot reindex on an axis with duplicate labels\n",
      "Processing summary:\n",
      "- Processed 44 subjects\n",
      "- Total days: 521\n",
      "- Total images: 3172\n",
      "- Total meals: 1689\n",
      "Full dataset size: 499\n",
      "Train dataset size: 399\n",
      "Test dataset size: 100\n",
      "Done creating train and test datasets\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_calorie_stats(data_loader):\n",
    "    \"\"\"\n",
    "    Computes the mean and std of daily total calories across the dataset.\n",
    "    \n",
    "    Each batch is a dict with 'nutrition' key, which contains a list of days,\n",
    "    where each day is a list of meal dicts.\n",
    "    \"\"\"\n",
    "    total_calories_per_day = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        for day_meals in batch[\"nutrition\"]:  # each element is a day's meals\n",
    "            if isinstance(day_meals, list):\n",
    "                day_total = sum(meal.get(\"calories\", 0) for meal in day_meals if isinstance(meal, dict))\n",
    "                if day_total > 800:\n",
    "                    total_calories_per_day.append(day_total)\n",
    "\n",
    "    calories_tensor = torch.tensor(total_calories_per_day, dtype=torch.float32)\n",
    "    return calories_tensor.mean().item(), calories_tensor.std().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_calorie_stats_lunch(data_loader):\n",
    "    \"\"\"\n",
    "    Computes the mean and std of daily total calories and lunch calories across the dataset.\n",
    "    \n",
    "    Each batch is a dict with 'nutrition' key, which contains a list of days,\n",
    "    where each day is a list of meal dicts.\n",
    "    \n",
    "    Returns:\n",
    "        (total_mean, total_std), (lunch_mean, lunch_std)\n",
    "    \"\"\"\n",
    "    total_calories_per_day = []\n",
    "    lunch_calories_per_day = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        for day_meals in batch[\"nutrition\"]:  # each element is a day's meals\n",
    "            if isinstance(day_meals, list):\n",
    "                # Total calories\n",
    "                day_total = sum(meal.get(\"calories\", 0) for meal in day_meals if isinstance(meal, dict))\n",
    "                if day_total > 800:\n",
    "                    total_calories_per_day.append(day_total)\n",
    "\n",
    "                # Lunch-only calories\n",
    "                lunch_total = sum(\n",
    "                    meal.get(\"calories\", 0)\n",
    "                    for meal in day_meals\n",
    "                    if isinstance(meal, dict) and meal.get(\"MealType\") == \"lunch\"\n",
    "                )\n",
    "                if lunch_total > 0:\n",
    "                    lunch_calories_per_day.append(lunch_total)\n",
    "\n",
    "    total_tensor = torch.tensor(total_calories_per_day, dtype=torch.float32)\n",
    "    lunch_tensor = torch.tensor(lunch_calories_per_day, dtype=torch.float32)\n",
    "\n",
    "    return (total_tensor.mean().item(), total_tensor.std().item()), (\n",
    "        lunch_tensor.mean().item(), lunch_tensor.std().item()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, lunchTuple =  compute_calorie_stats_lunch(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573.459228515625"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_mean, global_std = compute_calorie_stats(train_loader)\n",
    "global_mean, global_std = float(global_mean), float(global_std)\n",
    "global_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1911.24658203125"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labels(labels,mean,std, device, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Processes labels by summing the 'calories' field for each instance in a batch.\n",
    "\n",
    "    Args:\n",
    "        labels (list of list of dicts): A batch of labels, where each instance is a list of meal records.\n",
    "        device (str): Target device (\"cuda:0\" or \"cpu\").\n",
    "        dtype (torch.dtype): Data type (default: float16 for mixed precision).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (batch_size,) with summed calories per instance.\n",
    "    \"\"\"\n",
    "    batch_calories = [\n",
    "        sum(entry.get(\"calories\", 0) for entry in instance)  \n",
    "        for instance in labels\n",
    "    ]\n",
    "    labels_tensor = torch.tensor(batch_calories, dtype=dtype).to(device)\n",
    "    normalized_labels = (labels_tensor - mean) / (std + 1e-8)\n",
    "    \n",
    "    return normalized_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # Import torch.nn\n",
    "import torch.nn.functional as F \n",
    "class RMSRELoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon  # Small value to avoid division by zero\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        relative_error = (pred - target) / (target + self.epsilon)\n",
    "        squared_rel_error = relative_error ** 2\n",
    "        mean_squared_rel_error = torch.mean(squared_rel_error)\n",
    "        return torch.sqrt(mean_squared_rel_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['subject_ids', 'days', 'cgm_data', 'activity_data', 'images', 'nutrition', 'subject_day_pairs', 'timestamps', 'meal_timing_features', 'demographics'])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1440])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['meal_timing_features'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labels_lunch_only(labels, mean, std, device, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Processes labels by summing the 'calories' of 'lunch' meals for each instance in a batch.\n",
    "\n",
    "    Args:\n",
    "        labels (list of list of dicts): A batch of labels, where each instance is a list of meal records.\n",
    "        mean (float): Mean used for normalization.\n",
    "        std (float): Standard deviation used for normalization.\n",
    "        device (str): Target device (\"cuda:0\" or \"cpu\").\n",
    "        dtype (torch.dtype): Data type (default: float32).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (batch_size,) with summed lunch calories per instance.\n",
    "    \"\"\"\n",
    "    batch_calories = [\n",
    "        sum(entry.get(\"calories\", 0) for entry in instance if entry.get(\"MealType\") == \"lunch\")  \n",
    "        for instance in labels\n",
    "    ]\n",
    "    labels_tensor = torch.tensor(batch_calories, dtype=dtype).to(device)\n",
    "    normalized_labels = (labels_tensor - mean) / (std + 1e-8)\n",
    "\n",
    "    return normalized_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(\n",
    "    activity_encoder, cgm_encoder, meal_time_encoder, regressor, \n",
    "    train_loader, val_loader, global_mean, global_std,\n",
    "    device_activity=\"cuda:0\", device_cgm=\"cuda:1\", device_meal=\"cuda:0\", device_regressor=\"cuda:0\",\n",
    "    epochs=30, lr=5e-4, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with multiple encoders and demographics integration at regressor level.\n",
    "    \n",
    "    Args:\n",
    "        activity_encoder: Encoder for activity data\n",
    "        cgm_encoder: Encoder for CGM data\n",
    "        meal_time_encoder: Encoder for meal timing features\n",
    "        regressor: Final regressor that combines all features\n",
    "        train_loader, val_loader: Data loaders\n",
    "        global_mean, global_std: Normalization parameters\n",
    "        device_*: Device assignments for different components\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        device: Default device if specific devices not available\n",
    "    \"\"\"\n",
    "    # Check device availability and set device configuration\n",
    "    if not torch.cuda.is_available() and \"cuda\" in (device_activity, device_cgm, device_meal, device_regressor):\n",
    "        print(\"CUDA not available. Falling back to CPU.\")\n",
    "        device_activity = device_cgm = device_meal = device_regressor = \"cpu\"\n",
    "    elif torch.cuda.device_count() == 1 and any(d != \"cuda:0\" for d in [device_activity, device_cgm, device_meal, device_regressor]):\n",
    "        print(f\"Only one CUDA device available. Using cuda:0 for all components.\")\n",
    "        device_activity = device_cgm = device_meal = device_regressor = \"cuda:0\"\n",
    "    \n",
    "    # Move models to respective devices\n",
    "    activity_encoder.to(device_activity)\n",
    "    cgm_encoder.to(device_cgm)\n",
    "    meal_time_encoder.to(device_meal)\n",
    "    regressor.to(device_regressor)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = RMSRELoss()\n",
    "    \n",
    "    # Create a single optimizer for all parameters\n",
    "    optimizer = optim.Adam(\n",
    "        list(activity_encoder.parameters()) +\n",
    "        list(cgm_encoder.parameters()) +\n",
    "        list(meal_time_encoder.parameters()) +\n",
    "        list(regressor.parameters()), \n",
    "        lr=lr\n",
    "    )\n",
    "    \n",
    "    # Store loss values\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # Determine if we can use mixed precision\n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs), ascii=True, desc=\"Training Epochs\"):\n",
    "        # Set models to training mode\n",
    "        activity_encoder.train()\n",
    "        cgm_encoder.train()\n",
    "        meal_time_encoder.train()\n",
    "        regressor.train()\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            # Extract and move data to respective devices\n",
    "            activity_data = batch[\"activity_data\"].to(device_activity)\n",
    "            cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device_cgm)\n",
    "            meal_timing_data = torch.stack(batch['meal_timing_features'], dim=0).to(device_meal)\n",
    "            demographics = batch['demographics'].to(device_regressor)\n",
    "            \n",
    "            # Process labels\n",
    "            labels = process_labels(batch[\"nutrition\"], global_mean, global_std, device_regressor)\n",
    "            labels = labels.float().to(device_regressor)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with optional mixed precision\n",
    "            if use_amp:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    # Generate embeddings from each encoder\n",
    "                    activity_emb = activity_encoder(activity_data)\n",
    "                    cgm_emb = cgm_encoder(cgm_data)\n",
    "                    meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                    \n",
    "                    # Move embeddings to regressor device\n",
    "                    activity_emb = activity_emb.to(device_regressor)\n",
    "                    cgm_emb = cgm_emb.to(device_regressor)\n",
    "                    meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                    \n",
    "                    # Concatenate embeddings with demographics\n",
    "                    joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, demographics], dim=1).to(torch.float32)\n",
    "                    \n",
    "                    # Final prediction\n",
    "                    pred = regressor(joint_emb).squeeze(1)\n",
    "                   \n",
    "                    # Compute loss\n",
    "                    loss = criterion(pred, labels).mean(dim=0)\n",
    "                \n",
    "                # Backpropagation with scaler\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard processing without mixed precision\n",
    "                activity_emb = activity_encoder(activity_data)\n",
    "                cgm_emb = cgm_encoder(cgm_data)\n",
    "                meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                \n",
    "                # Move embeddings to regressor device\n",
    "                activity_emb = activity_emb.to(device_regressor)\n",
    "                cgm_emb = cgm_emb.to(device_regressor)\n",
    "                meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                \n",
    "                # Concatenate embeddings with demographics\n",
    "                joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, demographics], dim=1).to(torch.float32)\n",
    "                \n",
    "                # Final prediction\n",
    "                pred = regressor(joint_emb)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(pred, labels).mean(dim=0)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation Loop\n",
    "        activity_encoder.eval()\n",
    "        cgm_encoder.eval()\n",
    "        meal_time_encoder.eval()\n",
    "        regressor.eval()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Extract and move data to respective devices\n",
    "                activity_data = batch[\"activity_data\"].to(device_activity)\n",
    "                cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device_cgm)\n",
    "                meal_timing_data = torch.stack(batch['meal_timing_features'], dim=0).to(device_meal)\n",
    "                demographics = batch['demographics'].to(device_regressor)\n",
    "                \n",
    "                # Process labels\n",
    "                labels = process_labels(batch[\"nutrition\"], global_mean, global_std, device_regressor)\n",
    "                labels = labels.float().to(device_regressor)\n",
    "                \n",
    "                # Generate embeddings\n",
    "                activity_emb = activity_encoder(activity_data)\n",
    "                cgm_emb = cgm_encoder(cgm_data)\n",
    "                meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                \n",
    "                # Move embeddings to regressor device\n",
    "                activity_emb = activity_emb.to(device_regressor)\n",
    "                cgm_emb = cgm_emb.to(device_regressor)\n",
    "                meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                \n",
    "                # Concatenate embeddings with demographics\n",
    "                joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, demographics], dim=1).to(torch.float32)\n",
    "                \n",
    "                # Final prediction\n",
    "                pred = regressor(joint_emb)\n",
    "                # print(f\"Prdiction :{pred[0]} \")\n",
    "                # print(f\"Label :{labels[0]} \")   \n",
    "                # Compute loss\n",
    "                loss = criterion(pred, labels).mean(dim=0)\n",
    "                val_loss += loss.item()\n",
    "\n",
    " \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            validation_losses.append(avg_val_loss)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training Complete!\")\n",
    "    return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def train_model(\n",
    "    activity_encoder, cgm_encoder, meal_time_encoder, regressor, \n",
    "    train_loader, val_loader, global_mean, global_std,\n",
    "    device_activity=\"cuda:0\", device_cgm=\"cuda:1\", device_meal=\"cuda:0\", device_regressor=\"cuda:0\",\n",
    "    epochs=30, lr=5e-4, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with multiple encoders and demographics integration at regressor level.\n",
    "    \n",
    "    Args:\n",
    "        activity_encoder: Encoder for activity data\n",
    "        cgm_encoder: Encoder for CGM data\n",
    "        meal_time_encoder: Encoder for meal timing features\n",
    "        regressor: Final regressor that combines all features\n",
    "        train_loader, val_loader: Data loaders\n",
    "        global_mean, global_std: Normalization parameters\n",
    "        device_*: Device assignments for different components\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        device: Default device if specific devices not available\n",
    "    \"\"\"\n",
    "    # Check device availability and set device configuration\n",
    "    if not torch.cuda.is_available() and \"cuda\" in (device_activity, device_cgm, device_meal, device_regressor):\n",
    "        print(\"CUDA not available. Falling back to CPU.\")\n",
    "        device_activity = device_cgm = device_meal = device_regressor = \"cpu\"\n",
    "    elif torch.cuda.device_count() == 1 and any(d != \"cuda:0\" for d in [device_activity, device_cgm, device_meal, device_regressor]):\n",
    "        print(f\"Only one CUDA device available. Using cuda:0 for all components.\")\n",
    "        device_activity = device_cgm = device_meal = device_regressor = \"cuda:0\"\n",
    "    \n",
    "    # Move models to respective devices\n",
    "    activity_encoder.to(device_activity)\n",
    "    cgm_encoder.to(device_cgm)\n",
    "    meal_time_encoder.to(device_meal)\n",
    "    regressor.to(device_regressor)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = RMSRELoss()\n",
    "    \n",
    "    # Create a single optimizer for all parameters\n",
    "    optimizer = optim.AdamW(\n",
    "        list(activity_encoder.parameters()) +\n",
    "        list(cgm_encoder.parameters()) +\n",
    "        list(meal_time_encoder.parameters()) +\n",
    "        list(regressor.parameters()), \n",
    "        lr=lr,\n",
    "        weight_decay=1e-5  # L2 regularization\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Store loss values and metrics\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    pearson_correlations = []\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    best_correlation = -float('inf')\n",
    "    patience = 7\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Determine if we can use mixed precision\n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs), ascii=True, desc=\"Training Epochs\"):\n",
    "        # Set models to training mode\n",
    "        activity_encoder.train()\n",
    "        cgm_encoder.train()\n",
    "        meal_time_encoder.train()\n",
    "        regressor.train()\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Extract and move data to respective devices\n",
    "            activity_data = batch[\"activity_data\"].to(device_activity)\n",
    "            cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device_cgm)\n",
    "            meal_timing_data = torch.stack(batch['meal_timing_features'], dim=0).to(device_meal)\n",
    "            demographics = batch['demographics'].to(device_regressor)\n",
    "            \n",
    "            # Process labels\n",
    "            labels = process_labels(batch[\"nutrition\"], global_mean, global_std, device_regressor)\n",
    "            labels = labels.float().to(device_regressor)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with optional mixed precision\n",
    "            if use_amp:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    # Generate embeddings from each encoder\n",
    "                    activity_emb = activity_encoder(activity_data)\n",
    "                    cgm_emb = cgm_encoder(cgm_data)\n",
    "                    meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                    \n",
    "                    # Move embeddings to regressor device\n",
    "                    activity_emb = activity_emb.to(device_regressor)\n",
    "                    cgm_emb = cgm_emb.to(device_regressor)\n",
    "                    meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                    \n",
    "                    # Concatenate embeddings with demographics\n",
    "                    joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, demographics], dim=1).to(torch.float32)\n",
    "                    \n",
    "                    # Final prediction\n",
    "                    pred = regressor(joint_emb).squeeze(1)\n",
    "                   \n",
    "                    # Compute loss\n",
    "                    loss = criterion(pred, labels).mean(dim=0)\n",
    "                \n",
    "                # Backpropagation with scaler\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(activity_encoder.parameters()) +\n",
    "                    list(cgm_encoder.parameters()) +\n",
    "                    list(meal_time_encoder.parameters()) +\n",
    "                    list(regressor.parameters()), \n",
    "                    max_norm=1.0\n",
    "                )\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard processing without mixed precision\n",
    "                activity_emb = activity_encoder(activity_data)\n",
    "                cgm_emb = cgm_encoder(cgm_data)\n",
    "                meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                \n",
    "                # Move embeddings to regressor device\n",
    "                activity_emb = activity_emb.to(device_regressor)\n",
    "                cgm_emb = cgm_emb.to(device_regressor)\n",
    "                meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                \n",
    "                # Concatenate embeddings with demographics\n",
    "                joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, demographics], dim=1).to(torch.float32)\n",
    "                \n",
    "                # Final prediction\n",
    "                pred = regressor(joint_emb).squeeze(1)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(pred, labels).mean(dim=0)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(activity_encoder.parameters()) +\n",
    "                    list(cgm_encoder.parameters()) +\n",
    "                    list(meal_time_encoder.parameters()) +\n",
    "                    list(regressor.parameters()), \n",
    "                    max_norm=1.0\n",
    "                )\n",
    "                \n",
    "                optimizer.step()\n",
    "            \n",
    "            # Print diagnostics for first batch of first epoch\n",
    "            if epoch == 0 and batch_idx == 0:\n",
    "                print(f\"Sample predictions: {pred[:5]}\")\n",
    "                print(f\"Sample labels: {labels[:5]}\")\n",
    "                print(f\"Initial loss value: {loss.item()}\")\n",
    "                \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation Loop\n",
    "        activity_encoder.eval()\n",
    "        cgm_encoder.eval()\n",
    "        meal_time_encoder.eval()\n",
    "        regressor.eval()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Extract and move data to respective devices\n",
    "                activity_data = batch[\"activity_data\"].to(device_activity)\n",
    "                cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device_cgm)\n",
    "                meal_timing_data = torch.stack(batch['meal_timing_features'], dim=0).to(device_meal)\n",
    "                demographics = batch['demographics'].to(device_regressor)\n",
    "                \n",
    "                # Process labels\n",
    "                labels = process_labels(batch[\"nutrition\"], global_mean, global_std, device_regressor)\n",
    "                labels = labels.float().to(device_regressor)\n",
    "                \n",
    "                # Generate embeddings\n",
    "                activity_emb = activity_encoder(activity_data)\n",
    "                cgm_emb = cgm_encoder(cgm_data)\n",
    "                meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                \n",
    "                # Move embeddings to regressor device\n",
    "                activity_emb = activity_emb.to(device_regressor)\n",
    "                cgm_emb = cgm_emb.to(device_regressor)\n",
    "                meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                \n",
    "                # Concatenate embeddings with demographics\n",
    "                joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, demographics], dim=1).to(torch.float32)\n",
    "                \n",
    "                # Final prediction\n",
    "                pred = regressor(joint_emb).squeeze(1)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(pred, labels).mean(dim=0)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Store predictions and labels for correlation calculation\n",
    "                all_preds.extend(pred.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            validation_losses.append(avg_val_loss)\n",
    "            \n",
    "            # Calculate Pearson correlation\n",
    "            # Convert back to original scale for better interpretability\n",
    "            all_preds_denorm = np.array(all_preds) * global_std + global_mean\n",
    "            all_labels_denorm = np.array(all_labels) * global_std + global_mean\n",
    "            \n",
    "            try:\n",
    "                correlation, p_value = pearsonr(all_preds_denorm, all_labels_denorm)\n",
    "                pearson_correlations.append(correlation)\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}, Pearson Correlation: {correlation:.4f} (p={p_value:.4f})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not calculate correlation: {str(e)}\")\n",
    "                correlation = 0\n",
    "                pearson_correlations.append(correlation)\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}, Pearson Correlation: N/A\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Early stopping check - consider both loss and correlation\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'activity_encoder': activity_encoder.state_dict(),\n",
    "                'cgm_encoder': cgm_encoder.state_dict(),\n",
    "                'meal_time_encoder': meal_time_encoder.state_dict(),\n",
    "                'regressor': regressor.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'correlation': correlation if 'correlation' in locals() else 0\n",
    "            }, 'best_model_by_loss_checkpoint.pth')\n",
    "            print(f\"Saved new best model (by loss) with validation loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Also save model with best correlation\n",
    "        if 'correlation' in locals() and correlation > best_correlation:\n",
    "            best_correlation = correlation\n",
    "            torch.save({\n",
    "                'activity_encoder': activity_encoder.state_dict(),\n",
    "                'cgm_encoder': cgm_encoder.state_dict(),\n",
    "                'meal_time_encoder': meal_time_encoder.state_dict(),\n",
    "                'regressor': regressor.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'correlation': correlation\n",
    "            }, 'best_model_by_correlation_checkpoint.pth')\n",
    "            print(f\"Saved new best model (by correlation) with Pearson r: {correlation:.4f}\")\n",
    "        \n",
    "        # Update patience counter for early stopping\n",
    "        if avg_val_loss >= best_val_loss:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        # Optional: print current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current learning rate: {current_lr}\")\n",
    "    \n",
    "    print(\"Training Complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best Pearson correlation: {best_correlation:.4f}\")\n",
    "    \n",
    "    # Create a plot of training and validation metrics\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Plot losses\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(training_losses, label='Training Loss')\n",
    "        plt.plot(validation_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot correlation\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(pearson_correlations, label='Pearson Correlation', color='green')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Pearson r')\n",
    "        plt.title('Validation Correlation')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        print(\"Training metrics plot saved to 'training_metrics.png'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create plot: {str(e)}\")\n",
    "    \n",
    "    # Load best model based on validation loss\n",
    "    try:\n",
    "        best_checkpoint = torch.load('best_model_by_loss_checkpoint.pth')\n",
    "        activity_encoder.load_state_dict(best_checkpoint['activity_encoder'])\n",
    "        cgm_encoder.load_state_dict(best_checkpoint['cgm_encoder'])\n",
    "        meal_time_encoder.load_state_dict(best_checkpoint['meal_time_encoder'])\n",
    "        regressor.load_state_dict(best_checkpoint['regressor'])\n",
    "        print(f\"Loaded best model by loss from epoch {best_checkpoint['epoch']+1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load best model: {str(e)}\")\n",
    "    \n",
    "    return {\n",
    "        'training_losses': training_losses,\n",
    "        'validation_losses': validation_losses,\n",
    "        'pearson_correlations': pearson_correlations,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_correlation': best_correlation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model_all_modalities(\n",
    "    activity_encoder, cgm_encoder, meal_time_encoder, image_encoder, regressor, \n",
    "    train_loader, val_loader, global_mean, global_std,\n",
    "    device_activity=\"cuda:0\", device_cgm=\"cuda:1\", device_meal=\"cuda:0\", \n",
    "    device_image=\"cuda:1\", device_regressor=\"cuda:0\",\n",
    "    epochs=30, lr=5e-4, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with multiple encoders including image processing and demographics integration at regressor level.\n",
    "    \n",
    "    Args:\n",
    "        activity_encoder: Encoder for activity data\n",
    "        cgm_encoder: Encoder for CGM data\n",
    "        meal_time_encoder: Encoder for meal timing features\n",
    "        image_encoder: Encoder for food images\n",
    "        regressor: Final regressor that combines all features\n",
    "        train_loader, val_loader: Data loaders\n",
    "        global_mean, global_std: Normalization parameters\n",
    "        device_*: Device assignments for different components\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        device: Default device if specific devices not available\n",
    "    \"\"\"\n",
    "    # Check device availability and set device configuration\n",
    "    if not torch.cuda.is_available() and \"cuda\" in (device_activity, device_cgm, device_meal, device_image, device_regressor):\n",
    "        print(\"CUDA not available. Falling back to CPU.\")\n",
    "        device_activity = device_cgm = device_meal = device_image = device_regressor = \"cpu\"\n",
    "    elif torch.cuda.device_count() == 1 and any(d != \"cuda:0\" for d in [device_activity, device_cgm, device_meal, device_image, device_regressor]):\n",
    "        print(f\"Only one CUDA device available. Using cuda:0 for all components.\")\n",
    "        device_activity = device_cgm = device_meal = device_image = device_regressor = \"cuda:0\"\n",
    "    \n",
    "    # Move models to respective devices\n",
    "    activity_encoder.to(device_activity)\n",
    "    cgm_encoder.to(device_cgm)\n",
    "    meal_time_encoder.to(device_meal)\n",
    "    image_encoder.to(device_image)\n",
    "    regressor.to(device_regressor)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = RMSRELoss()\n",
    "    \n",
    "    # Create a single optimizer for all parameters\n",
    "    optimizer = optim.Adam(\n",
    "        list(activity_encoder.parameters()) +\n",
    "        list(cgm_encoder.parameters()) +\n",
    "        list(meal_time_encoder.parameters()) +\n",
    "        list(image_encoder.parameters()) +\n",
    "        list(regressor.parameters()), \n",
    "        lr=lr\n",
    "    )\n",
    "    \n",
    "    # Store loss values\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # Determine if we can use mixed precision\n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs), ascii=True, desc=\"Training Epochs\"):\n",
    "        # Set models to training mode\n",
    "        activity_encoder.train()\n",
    "        cgm_encoder.train()\n",
    "        meal_time_encoder.train()\n",
    "        image_encoder.train()\n",
    "        regressor.train()\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            # Extract and move data to respective devices\n",
    "            activity_data = batch[\"activity_data\"].to(device_activity)\n",
    "            cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device_cgm)\n",
    "            meal_timing_data = torch.stack(batch['meal_timing_features'], dim=0).to(device_meal)\n",
    "            demographics = batch['demographics'].to(device_regressor)\n",
    "            \n",
    "            # Process image data - handle variable number of images\n",
    "            batch_images = batch[\"images\"]  # List of lists of images\n",
    "            batch_image_embeddings = []\n",
    "            \n",
    "            for sample_images in batch_images:\n",
    "                if len(sample_images) > 0:\n",
    "                    # Move each image to the image device and process\n",
    "                    sample_image_tensors = [img.to(device_image) for img in sample_images]\n",
    "                    # Process each image and get embeddings\n",
    "                    sample_image_embs = [image_encoder(img.unsqueeze(0)) for img in sample_image_tensors]\n",
    "                    # Average all image embeddings for this sample\n",
    "                    if sample_image_embs:\n",
    "                        avg_image_emb = torch.mean(torch.stack(sample_image_embs), dim=0)\n",
    "                    else:\n",
    "                        # Fallback if no valid images (should not happen if len > 0)\n",
    "                        avg_image_emb = torch.zeros(image_encoder.output_dim, device=device_image)\n",
    "                else:\n",
    "                    # No images for this sample, use zero embedding\n",
    "                    avg_image_emb = torch.zeros(image_encoder.output_dim, device=device_image)\n",
    "                \n",
    "                batch_image_embeddings.append(avg_image_emb)\n",
    "            \n",
    "            # Stack all image embeddings for the batch\n",
    "            image_embs = torch.stack(batch_image_embeddings)\n",
    "            image_embs = image_embs.to(device_regressor)\n",
    "            \n",
    "            # Process labels\n",
    "            labels = process_labels(batch[\"nutrition\"], global_mean, global_std, device_regressor)\n",
    "            labels = labels.float().to(device_regressor)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with optional mixed precision\n",
    "            if use_amp:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    # Generate embeddings from each encoder\n",
    "                    activity_emb = activity_encoder(activity_data)\n",
    "                    cgm_emb = cgm_encoder(cgm_data)\n",
    "                    meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                    \n",
    "                    # Move embeddings to regressor device\n",
    "                    activity_emb = activity_emb.to(device_regressor)\n",
    "                    cgm_emb = cgm_emb.to(device_regressor)\n",
    "                    meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                    \n",
    "                    # Concatenate all embeddings with demographics\n",
    "                    joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, image_embs, demographics], dim=1).to(torch.float32)\n",
    "                    \n",
    "                    # Final prediction\n",
    "                    pred = regressor(joint_emb).squeeze(1)\n",
    "                   \n",
    "                    # Compute loss\n",
    "                    loss = criterion(pred, labels).mean(dim=0)\n",
    "                \n",
    "                # Backpropagation with scaler\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard processing without mixed precision\n",
    "                activity_emb = activity_encoder(activity_data)\n",
    "                cgm_emb = cgm_encoder(cgm_data)\n",
    "                meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                \n",
    "                # Move embeddings to regressor device\n",
    "                activity_emb = activity_emb.to(device_regressor)\n",
    "                cgm_emb = cgm_emb.to(device_regressor)\n",
    "                meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                \n",
    "                # Concatenate all embeddings with demographics\n",
    "                joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, image_embs, demographics], dim=1).to(torch.float32)\n",
    "                \n",
    "                # Final prediction\n",
    "                pred = regressor(joint_emb)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(pred, labels).mean(dim=0)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation Loop\n",
    "        activity_encoder.eval()\n",
    "        cgm_encoder.eval()\n",
    "        meal_time_encoder.eval()\n",
    "        image_encoder.eval()\n",
    "        regressor.eval()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Extract and move data to respective devices\n",
    "                activity_data = batch[\"activity_data\"].to(device_activity)\n",
    "                cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device_cgm)\n",
    "                meal_timing_data = torch.stack(batch['meal_timing_features'], dim=0).to(device_meal)\n",
    "                demographics = batch['demographics'].to(device_regressor)\n",
    "                \n",
    "                # Process image data - handle variable number of images\n",
    "                batch_images = batch[\"images\"]  # List of lists of images\n",
    "                batch_image_embeddings = []\n",
    "                \n",
    "                for sample_images in batch_images:\n",
    "                    if len(sample_images) > 0:\n",
    "                        # Move each image to the image device and process\n",
    "                        sample_image_tensors = [img.to(device_image) for img in sample_images]\n",
    "                        # Process each image and get embeddings\n",
    "                        sample_image_embs = [image_encoder(img.unsqueeze(0)) for img in sample_image_tensors]\n",
    "                        # Average all image embeddings for this sample\n",
    "                        if sample_image_embs:\n",
    "                            avg_image_emb = torch.mean(torch.stack(sample_image_embs), dim=0)\n",
    "                        else:\n",
    "                            # Fallback if no valid images (should not happen if len > 0)\n",
    "                            avg_image_emb = torch.zeros(image_encoder.output_dim, device=device_image)\n",
    "                    else:\n",
    "                        # No images for this sample, use zero embedding\n",
    "                        avg_image_emb = torch.zeros(image_encoder.output_dim, device=device_image)\n",
    "                    \n",
    "                    batch_image_embeddings.append(avg_image_emb)\n",
    "                \n",
    "                # Stack all image embeddings for the batch\n",
    "                image_embs = torch.stack(batch_image_embeddings)\n",
    "                image_embs = image_embs.to(device_regressor)\n",
    "                \n",
    "                # Process labels\n",
    "                labels = process_labels(batch[\"nutrition\"], global_mean, global_std, device_regressor)\n",
    "                labels = labels.float().to(device_regressor)\n",
    "                \n",
    "                # Generate embeddings\n",
    "                activity_emb = activity_encoder(activity_data)\n",
    "                cgm_emb = cgm_encoder(cgm_data)\n",
    "                meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                \n",
    "                # Move embeddings to regressor device\n",
    "                activity_emb = activity_emb.to(device_regressor)\n",
    "                cgm_emb = cgm_emb.to(device_regressor)\n",
    "                meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                \n",
    "                # Concatenate all embeddings with demographics\n",
    "                joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, image_embs, demographics], dim=1).to(torch.float32)\n",
    "                \n",
    "                # Final prediction\n",
    "                pred = regressor(joint_emb)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(pred, labels).mean(dim=0)\n",
    "                val_loss += loss.item()\n",
    " \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            validation_losses.append(avg_val_loss)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training Complete!\")\n",
    "    return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_embeddings(batch_images, image_encoder, device_image, device_regressor):\n",
    "    \"\"\"\n",
    "    Process batch of images and generate embeddings with consistent shape.\n",
    "    Handles both PyTorch tensors and NumPy arrays as input.\n",
    "    \"\"\"\n",
    "    batch_image_embeddings = []\n",
    "\n",
    "    for sample_images in batch_images:\n",
    "        if len(sample_images) > 0:\n",
    "            # Convert NumPy arrays to PyTorch tensors and handle formatting\n",
    "            sample_image_tensors = []\n",
    "            for img in sample_images:\n",
    "                # Convert to tensor\n",
    "                img_tensor = torch.tensor(img, dtype=torch.float32)\n",
    "                \n",
    "                # Ensure image is in channel-first format (C×H×W)\n",
    "                if img_tensor.shape[-1] == 3:  # If channel is last dimension (H×W×C)\n",
    "                    img_tensor = img_tensor.permute(2, 0, 1)  # Convert to C×H×W\n",
    "                \n",
    "                # Move to device\n",
    "                img_tensor = img_tensor.to(device_image)\n",
    "                sample_image_tensors.append(img_tensor)\n",
    "            \n",
    "            # Process each image and get embeddings\n",
    "            sample_image_embs = [image_encoder(img.unsqueeze(0)) for img in sample_image_tensors]\n",
    "            \n",
    "            # Average all image embeddings for this sample\n",
    "            if sample_image_embs:\n",
    "                avg_image_emb = torch.mean(torch.stack(sample_image_embs), dim=0)\n",
    "            else:\n",
    "                avg_image_emb = torch.zeros(image_encoder.output_dim, device=device_image)\n",
    "        else:\n",
    "            # No images for this sample, use zero embedding\n",
    "            avg_image_emb = torch.zeros(image_encoder.output_dim, device=device_image)\n",
    "        \n",
    "        # Add this sample's embedding to the batch\n",
    "        batch_image_embeddings.append(avg_image_emb)\n",
    "    \n",
    "    # Stack all image embeddings for the batch\n",
    "    image_embs = torch.stack(batch_image_embeddings)\n",
    "    \n",
    "    # Remove any extra dimensions that might be causing shape issues\n",
    "    if len(image_embs.shape) > 2:\n",
    "        image_embs = image_embs.squeeze(1)  # Remove any singleton dimensions\n",
    "        \n",
    "    image_embs = image_embs.to(device_regressor)\n",
    "    \n",
    "    return image_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model_all_modalities(\n",
    "    activity_encoder, cgm_encoder, meal_time_encoder, image_encoder, regressor, \n",
    "    train_loader, val_loader, global_mean, global_std,\n",
    "    device_activity=\"cuda:0\", device_cgm=\"cuda:1\", device_meal=\"cuda:0\", \n",
    "    device_image=\"cuda:1\", device_regressor=\"cuda:0\",\n",
    "    epochs=30, lr=5e-4, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with multiple encoders including image processing and demographics integration at regressor level.\n",
    "    \n",
    "    Args:\n",
    "        activity_encoder: Encoder for activity data\n",
    "        cgm_encoder: Encoder for CGM data\n",
    "        meal_time_encoder: Encoder for meal timing features\n",
    "        image_encoder: Encoder for food images\n",
    "        regressor: Final regressor that combines all features\n",
    "        train_loader, val_loader: Data loaders\n",
    "        global_mean, global_std: Normalization parameters\n",
    "        device_*: Device assignments for different components\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        device: Default device if specific devices not available\n",
    "    \"\"\"\n",
    "    # Check device availability and set device configuration\n",
    "    if not torch.cuda.is_available() and \"cuda\" in (device_activity, device_cgm, device_meal, device_image, device_regressor):\n",
    "        print(\"CUDA not available. Falling back to CPU.\")\n",
    "        device_activity = device_cgm = device_meal = device_image = device_regressor = \"cpu\"\n",
    "    elif torch.cuda.device_count() == 1 and any(d != \"cuda:0\" for d in [device_activity, device_cgm, device_meal, device_image, device_regressor]):\n",
    "        print(f\"Only one CUDA device available. Using cuda:0 for all components.\")\n",
    "        device_activity = device_cgm = device_meal = device_image = device_regressor = \"cuda:0\"\n",
    "    \n",
    "    # Move models to respective devices\n",
    "    activity_encoder.to(device_activity)\n",
    "    cgm_encoder.to(device_cgm)\n",
    "    meal_time_encoder.to(device_meal)\n",
    "    image_encoder.to(device_image)\n",
    "    regressor.to(device_regressor)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = RMSRELoss()\n",
    "    \n",
    "    # Create a single optimizer for all parameters\n",
    "    optimizer = optim.Adam(\n",
    "        list(activity_encoder.parameters()) +\n",
    "        list(cgm_encoder.parameters()) +\n",
    "        list(meal_time_encoder.parameters()) +\n",
    "        list(image_encoder.parameters()) +\n",
    "        list(regressor.parameters()), \n",
    "        lr=lr\n",
    "    )\n",
    "    \n",
    "    # Store loss values\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # Determine if we can use mixed precision\n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs), ascii=True, desc=\"Training Epochs\"):\n",
    "        # Set models to training mode\n",
    "        activity_encoder.train()\n",
    "        cgm_encoder.train()\n",
    "        meal_time_encoder.train()\n",
    "        image_encoder.train()\n",
    "        regressor.train()\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            # Extract and move data to respective devices\n",
    "            activity_data = batch[\"activity_data\"].to(device_activity)\n",
    "            cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device_cgm)\n",
    "            meal_timing_data = torch.stack(batch['meal_timing_features'], dim=0).to(device_meal)\n",
    "            demographics = batch['demographics'].to(device_regressor)\n",
    "            \n",
    "            # Process image data using the dedicated function\n",
    "            image_embs = process_image_embeddings(batch[\"images\"], image_encoder, device_image, device_regressor)\n",
    "            \n",
    "            # Process labels\n",
    "            labels = process_labels(batch[\"nutrition\"], global_mean, global_std, device_regressor)\n",
    "            labels = labels.float().to(device_regressor)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with optional mixed precision\n",
    "            if use_amp:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    # Generate embeddings from each encoder\n",
    "                    activity_emb = activity_encoder(activity_data)\n",
    "                    cgm_emb = cgm_encoder(cgm_data)\n",
    "                    meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                    \n",
    "                    # Move embeddings to regressor device and ensure consistent shape\n",
    "                    activity_emb = activity_emb.to(device_regressor)\n",
    "                    cgm_emb = cgm_emb.to(device_regressor)\n",
    "                    meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                    \n",
    "                    # Ensure all embeddings have the same first dimension (batch_size)\n",
    "                    batch_size = activity_emb.size(0)\n",
    "                    assert cgm_emb.size(0) == batch_size, \"CGM embedding batch dimension mismatch\"\n",
    "                    assert meal_time_emb.size(0) == batch_size, \"Meal timing embedding batch dimension mismatch\"\n",
    "                    assert image_embs.size(0) == batch_size, \"Image embedding batch dimension mismatch\"\n",
    "                    assert demographics.size(0) == batch_size, \"Demographics batch dimension mismatch\"\n",
    "                    \n",
    "                    # Concatenate all embeddings with demographics\n",
    "                    joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, image_embs, demographics], dim=1).to(torch.float32)\n",
    "                    \n",
    "                    # Final prediction\n",
    "                    pred = regressor(joint_emb).squeeze(1)\n",
    "                   \n",
    "                    # Compute loss\n",
    "                    loss = criterion(pred, labels).mean(dim=0)\n",
    "                \n",
    "                # Backpropagation with scaler\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard processing without mixed precision\n",
    "                activity_emb = activity_encoder(activity_data)\n",
    "                cgm_emb = cgm_encoder(cgm_data)\n",
    "                meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                \n",
    "                # Move embeddings to regressor device and ensure consistent shape\n",
    "                activity_emb = activity_emb.to(device_regressor)\n",
    "                cgm_emb = cgm_emb.to(device_regressor)\n",
    "                meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                \n",
    "                # Ensure all embeddings have the same first dimension (batch_size)\n",
    "                batch_size = activity_emb.size(0)\n",
    "                assert cgm_emb.size(0) == batch_size, \"CGM embedding batch dimension mismatch\"\n",
    "                assert meal_time_emb.size(0) == batch_size, \"Meal timing embedding batch dimension mismatch\"\n",
    "                assert image_embs.size(0) == batch_size, \"Image embedding batch dimension mismatch\"\n",
    "                assert demographics.size(0) == batch_size, \"Demographics batch dimension mismatch\"\n",
    "                \n",
    "                # Concatenate all embeddings with demographics\n",
    "                joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, image_embs, demographics], dim=1).to(torch.float32)\n",
    "                \n",
    "                # Final prediction\n",
    "                pred = regressor(joint_emb)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(pred, labels).mean(dim=0)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation Loop\n",
    "        activity_encoder.eval()\n",
    "        cgm_encoder.eval()\n",
    "        meal_time_encoder.eval()\n",
    "        image_encoder.eval()\n",
    "        regressor.eval()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Extract and move data to respective devices\n",
    "                activity_data = batch[\"activity_data\"].to(device_activity)\n",
    "                cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device_cgm)\n",
    "                meal_timing_data = torch.stack(batch['meal_timing_features'], dim=0).to(device_meal)\n",
    "                demographics = batch['demographics'].to(device_regressor)\n",
    "                \n",
    "                # Process image data using the dedicated function\n",
    "                image_embs = process_image_embeddings(batch[\"images\"], image_encoder, device_image, device_regressor)\n",
    "                \n",
    "                # Process labels\n",
    "                labels = process_labels(batch[\"nutrition\"], global_mean, global_std, device_regressor)\n",
    "                labels = labels.float().to(device_regressor)\n",
    "                \n",
    "                # Generate embeddings\n",
    "                activity_emb = activity_encoder(activity_data)\n",
    "                cgm_emb = cgm_encoder(cgm_data)\n",
    "                meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                \n",
    "                # Move embeddings to regressor device and ensure consistent shape\n",
    "                activity_emb = activity_emb.to(device_regressor)\n",
    "                cgm_emb = cgm_emb.to(device_regressor)\n",
    "                meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                \n",
    "                # Ensure all embeddings have the same first dimension (batch_size)\n",
    "                batch_size = activity_emb.size(0)\n",
    "                assert cgm_emb.size(0) == batch_size, \"CGM embedding batch dimension mismatch\"\n",
    "                assert meal_time_emb.size(0) == batch_size, \"Meal timing embedding batch dimension mismatch\"\n",
    "                assert image_embs.size(0) == batch_size, \"Image embedding batch dimension mismatch\"\n",
    "                assert demographics.size(0) == batch_size, \"Demographics batch dimension mismatch\"\n",
    "                \n",
    "                # Concatenate all embeddings with demographics\n",
    "                joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, image_embs, demographics], dim=1).to(torch.float32)\n",
    "                \n",
    "                # Final prediction\n",
    "                pred = regressor(joint_emb)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(pred, labels).mean(dim=0)\n",
    "                val_loss += loss.item()\n",
    " \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            validation_losses.append(avg_val_loss)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training Complete!\")\n",
    "    return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import MultiheadAttention as TransformerEncoder\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageSetTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(ImageSetTransformer, self).__init__()\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Important to keep shape [batch, seq, dim]\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_dim = input_dim\n",
    "\n",
    "        # Optional: a learnable [CLS] token to summarize the sequence\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, input_dim]\n",
    "        mask: [batch_size, seq_len] -> True for tokens to be masked\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Add [CLS] token at the beginning\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # [B, 1, D]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # Now [B, 1+seq_len, D]\n",
    "\n",
    "        # Extend mask to account for CLS (CLS not masked)\n",
    "        if mask is not None:\n",
    "            cls_mask = torch.zeros(batch_size, 1, dtype=torch.bool, device=mask.device)\n",
    "            mask = torch.cat((cls_mask, mask), dim=1)\n",
    "\n",
    "        # Pass through Transformer\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "\n",
    "        # Return the CLS token output\n",
    "        return x[:, 0, :]  # [B, D]\n",
    "\n",
    "class CaloricRegressor(nn.Module):\n",
    "    def __init__(self, cgm_emb_size, activity_emb_size, meal_timing_emb_size, \n",
    "                 demographics_size=5, hidden_size=128, output_size=1):\n",
    "        super(CaloricRegressor, self).__init__()\n",
    "        \n",
    "        total_input_size = cgm_emb_size + activity_emb_size + meal_timing_emb_size + demographics_size\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(total_input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.regressor(x)\n",
    "    \n",
    "class MealTimingEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=5, hidden_size=64, output_size=32):\n",
    "        super(MealTimingEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, hidden_size, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv1d(hidden_size, hidden_size*2, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv1d(hidden_size*2, hidden_size*4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size*4, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, channels=5, seq_len=1440]\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze(-1)  # Remove the last dimension after global pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class CaloricModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CaloricModel, self).__init__()\n",
    "        self.cgm_encoder = TransformerEncoder(n_features=1440, embed_dim=96, num_heads=2, num_classes=64, dropout=0.2, num_layers=6)\n",
    "        self.activity_encoder = TransformerEncoder(n_features=1440, embed_dim=96, num_heads=2, num_classes=64, dropout=0.2, num_layers=6)\n",
    "        self.meal_timing_encoder = MealTimingEncoder(input_channels=5, hidden_size=64, output_size=32)\n",
    "        self.caloric_regressor = CaloricRegressor(\n",
    "            cgm_emb_size=64,\n",
    "            activity_emb_size=64,\n",
    "            meal_timing_emb_size=32,\n",
    "            demographics_size=5\n",
    "        )\n",
    "\n",
    "    def forward(self, cgm, activity, meal_timing, demographics):\n",
    "        cgm_embed = self.cgm_encoder(cgm)\n",
    "        activity_embed = self.activity_encoder(activity)\n",
    "        meal_embed = self.meal_timing_encoder(meal_timing)\n",
    "\n",
    "        # Concatenate all embeddings and demographic vector\n",
    "        x = torch.cat([cgm_embed, activity_embed, meal_embed, demographics], dim=1)\n",
    "        return self.caloric_regressor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1458027/1739250103.py:58: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
      "Training Epochs:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Training Loss: 0.9992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "Training Epochs:   3%|3         | 1/30 [01:26<41:55, 86.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Validation Loss: 0.9981\n",
      "Epoch [2/30], Training Loss: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   7%|6         | 2/30 [02:53<40:25, 86.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], Validation Loss: 0.9925\n",
      "Epoch [3/30], Training Loss: 0.9853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|#         | 3/30 [04:19<38:57, 86.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], Validation Loss: 0.9733\n",
      "Epoch [4/30], Training Loss: 0.9515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  13%|#3        | 4/30 [05:46<37:32, 86.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30], Validation Loss: 0.9193\n",
      "Epoch [5/30], Training Loss: 0.8718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  17%|#6        | 5/30 [07:13<36:10, 86.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Validation Loss: 0.7993\n",
      "Epoch [6/30], Training Loss: 0.7075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|##        | 6/30 [08:40<34:44, 86.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30], Validation Loss: 0.5732\n",
      "Epoch [7/30], Training Loss: 0.4441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  23%|##3       | 7/30 [10:08<33:22, 87.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], Validation Loss: 0.2919\n",
      "Epoch [8/30], Training Loss: 0.2830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  27%|##6       | 8/30 [11:34<31:49, 86.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30], Validation Loss: 0.2643\n",
      "Epoch [9/30], Training Loss: 0.2843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  30%|###       | 9/30 [13:00<30:20, 86.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30], Validation Loss: 0.2686\n",
      "Epoch [10/30], Training Loss: 0.2788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  33%|###3      | 10/30 [14:27<28:55, 86.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Validation Loss: 0.2640\n",
      "Epoch [11/30], Training Loss: 0.2712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  37%|###6      | 11/30 [15:54<27:30, 86.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], Validation Loss: 0.2668\n",
      "Epoch [12/30], Training Loss: 0.2729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|####      | 12/30 [17:22<26:06, 87.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30], Validation Loss: 0.2642\n",
      "Epoch [13/30], Training Loss: 0.2824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  43%|####3     | 13/30 [18:49<24:39, 87.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30], Validation Loss: 0.2662\n",
      "Epoch [14/30], Training Loss: 0.2716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  47%|####6     | 14/30 [20:15<23:09, 86.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30], Validation Loss: 0.2662\n",
      "Epoch [15/30], Training Loss: 0.2780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  50%|#####     | 15/30 [21:42<21:42, 86.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Validation Loss: 0.2659\n",
      "Epoch [16/30], Training Loss: 0.2763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  53%|#####3    | 16/30 [23:09<20:15, 86.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30], Validation Loss: 0.2659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  53%|#####3    | 16/30 [24:25<21:22, 91.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 46\u001b[0m\n\u001b[1;32m     35\u001b[0m regressor \u001b[38;5;241m=\u001b[39m Regressor(\n\u001b[1;32m     36\u001b[0m     input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m32\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m32\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m16\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m5\u001b[39m,  \u001b[38;5;66;03m# Sum of all embedding dimensions plus demographics\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     38\u001b[0m     output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# Adjust based on your nutrition prediction targets\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Create the final model\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#training_losses, validation_losses,pearson,_,_ = train_model(activity_encoder,cgm_encoder,meal_time_encoder,regressor,train_loader,test_loader,0,1,epochs=30,lr=1e-4)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m training_losses, validation_losses, pearson, pred_values, true_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_all_modalities\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivity_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivity_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcgm_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcgm_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeal_time_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeal_time_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregressor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregressor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 78\u001b[0m, in \u001b[0;36mtrain_model_all_modalities\u001b[0;34m(activity_encoder, cgm_encoder, meal_time_encoder, image_encoder, regressor, train_loader, val_loader, global_mean, global_std, device_activity, device_cgm, device_meal, device_image, device_regressor, epochs, lr, device)\u001b[0m\n\u001b[1;32m     75\u001b[0m demographics \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdemographics\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device_regressor)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Process image data using the dedicated function\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m image_embs \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_image_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_regressor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Process labels\u001b[39;00m\n\u001b[1;32m     81\u001b[0m labels \u001b[38;5;241m=\u001b[39m process_labels(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnutrition\u001b[39m\u001b[38;5;124m\"\u001b[39m], global_mean, global_std, device_regressor)\n",
      "Cell \u001b[0;32mIn[42], line 25\u001b[0m, in \u001b[0;36mprocess_image_embeddings\u001b[0;34m(batch_images, image_encoder, device_image, device_regressor)\u001b[0m\n\u001b[1;32m     22\u001b[0m     sample_image_tensors\u001b[38;5;241m.\u001b[39mappend(img_tensor)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Process each image and get embeddings\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m sample_image_embs \u001b[38;5;241m=\u001b[39m [image_encoder(img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m sample_image_tensors]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Average all image embeddings for this sample\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_image_embs:\n",
      "Cell \u001b[0;32mIn[42], line 25\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m     sample_image_tensors\u001b[38;5;241m.\u001b[39mappend(img_tensor)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Process each image and get embeddings\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m sample_image_embs \u001b[38;5;241m=\u001b[39m [\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m sample_image_tensors]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Average all image embeddings for this sample\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_image_embs:\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/CGM-ContinousData/transformer.py:28\u001b[0m, in \u001b[0;36mImageEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/vit_pytorch/vit.py:122\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    119\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding[:, :(n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m    120\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m--> 122\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    126\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_latent(x)\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/vit_pytorch/vit.py:79\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attn, ff \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     78\u001b[0m     x \u001b[38;5;241m=\u001b[39m attn(x) \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m---> 79\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/vit_pytorch/vit.py:27\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/grads/a/atkulkarni/miniconda3/envs/cgm_dinner/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformer import MultiheadAttention as TransformerEncoder\n",
    "from multiChannelTransformer import MultiChannelTransformerEncoder \n",
    "from joint_layers import Regressor\n",
    "from transformer import ImageEncoder\n",
    "\n",
    "image_encoder = ImageEncoder(\n",
    "    image_size=224,  # Standard input size for many vision models\n",
    "    patch_size=16,   # Typical patch size for ViT\n",
    "    num_classes=32,  # Match output dimension with other encoders\n",
    "    channels=3,      # RGB images\n",
    "    dropout=0.2      # Consistent with other components\n",
    "    \n",
    ")\n",
    "\n",
    "activity_encoder = MultiChannelTransformerEncoder(\n",
    "    n_features=1440,  # Your sequence length\n",
    "    n_channels=2,     # Number of channels in your tensor\n",
    "    embed_dim=96,\n",
    "    num_heads=2,\n",
    "    num_classes=32,\n",
    "    dropout=0.2,\n",
    "    num_layers=3\n",
    ")\n",
    "cgm_encoder = TransformerEncoder(n_features=1440, embed_dim=96, num_heads=2, num_classes=32, dropout=0.2, num_layers=3)\n",
    "meal_time_encoder = meal_time_encoder = MultiChannelTransformerEncoder(\n",
    "    n_features=1440,  # Your sequence length\n",
    "    n_channels=5,     # Number of channels in your tensor\n",
    "    embed_dim=96,\n",
    "    num_heads=2,\n",
    "    num_classes=16,\n",
    "    dropout=0.2,\n",
    "    num_layers=3\n",
    ")\n",
    "# Initialize regressor\n",
    "regressor = Regressor(\n",
    "    input_size=32+32+32+16+5,  # Sum of all embedding dimensions plus demographics\n",
    "    hidden=128,\n",
    "    output_size=1,  # Adjust based on your nutrition prediction targets\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "\n",
    "# Create the final model\n",
    "#training_losses, validation_losses,pearson,_,_ = train_model(activity_encoder,cgm_encoder,meal_time_encoder,regressor,train_loader,test_loader,0,1,epochs=30,lr=1e-4)\n",
    "\n",
    "training_losses, validation_losses, pearson, pred_values, true_values = train_model_all_modalities(\n",
    "        activity_encoder=activity_encoder,\n",
    "        cgm_encoder=cgm_encoder,\n",
    "        meal_time_encoder=meal_time_encoder,\n",
    "        image_encoder=image_encoder,\n",
    "        regressor=regressor,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=test_loader,\n",
    "        global_mean=0,\n",
    "        global_std=1,\n",
    "        epochs=30,\n",
    "        lr=1e-4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def collect_total_calories(train_loader):\n",
    "    \"\"\"\n",
    "    Collects total daily calories from the train_loader for histogram visualization.\n",
    "    \"\"\"\n",
    "    all_calories = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        nutrition = batch[\"nutrition\"]\n",
    "        for day_meals in nutrition:\n",
    "            if isinstance(day_meals, list):\n",
    "                day_total = sum(meal.get(\"calories\", 0) for meal in day_meals if isinstance(meal, dict))\n",
    "                if day_total > 800:  # Apply the same filter used in compute_calorie_stats\n",
    "                    all_calories.append(day_total)\n",
    "\n",
    "    return all_calories\n",
    "\n",
    "def plot_calorie_histogram(calories, bins=50):\n",
    "    \"\"\"\n",
    "    Plots a histogram of total daily calories.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(calories, bins=bins, color='skyblue', edgecolor='black')\n",
    "    plt.title(\"Distribution of Total Daily Calories\")\n",
    "    plt.xlabel(\"Total Calories\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "train_calories = collect_total_calories(train_loader)\n",
    "plot_calorie_histogram(train_calories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unified_model(\n",
    "    model, \n",
    "    train_loader, val_loader, \n",
    "    global_mean, global_std,\n",
    "    epochs=30, \n",
    "    lr=1e-3, \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a unified model with activity and CGM data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch.nn.Module\n",
    "        The unified model to train\n",
    "    train_loader : torch.utils.data.DataLoader\n",
    "        DataLoader for training data\n",
    "    val_loader : torch.utils.data.DataLoader\n",
    "        DataLoader for validation data\n",
    "    global_mean : float\n",
    "        Mean value for label normalization\n",
    "    global_std : float\n",
    "        Standard deviation for label normalization\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    lr : float\n",
    "        Learning rate for optimizer\n",
    "    device : str\n",
    "        Device to train on (\"cuda\" or \"cpu\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    training_losses : list\n",
    "        List of training losses per epoch\n",
    "    validation_losses : list\n",
    "        List of validation losses per epoch\n",
    "    \"\"\"\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define loss function\n",
    "    criterion = nn.MSELoss()  # or your custom RMSRELoss\n",
    "    \n",
    "    # Optimizer with weight decay for regularization\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Store loss values\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 7\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Determine if we can use mixed precision\n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    # Training loop\n",
    "    from tqdm import tqdm\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Extract data\n",
    "            activity_data = batch[\"activity_data\"].to(device)\n",
    "            cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device)\n",
    "            demographics = batch['demographics'].to(device)\n",
    "            \n",
    "            # Process labels\n",
    "            if isinstance(batch[\"nutrition\"], torch.Tensor):\n",
    "                labels = (batch[\"nutrition\"] - global_mean) / global_std\n",
    "            else:\n",
    "                # If nutrition is a list or dictionary, adjust accordingly\n",
    "                labels = process_labels(batch[\"nutrition\"], global_mean, global_std, device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with optional mixed precision\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # Forward pass through unified model\n",
    "                    pred = model(activity_data, cgm_data, demographics).squeeze(1)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = criterion(pred, labels)\n",
    "                \n",
    "                # Backpropagation with scaler\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient clipping to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Forward pass\n",
    "                pred = model(activity_data, cgm_data, demographics).squeeze(1)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(pred, labels)\n",
    "                \n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "            \n",
    "            # Print diagnostics for first batch of first epoch\n",
    "            if epoch == 0 and batch_idx == 0:\n",
    "                print(f\"Sample predictions: {pred[:5]}\")\n",
    "                print(f\"Sample labels: {labels[:5]}\")\n",
    "                print(f\"Initial loss value: {loss.item()}\")\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation Loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Extract data\n",
    "                activity_data = batch[\"activity_data\"].to(device)\n",
    "                cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device)\n",
    "                demographics = batch['demographics'].to(device)\n",
    "                \n",
    "                # Process labels\n",
    "                if isinstance(batch[\"nutrition\"], torch.Tensor):\n",
    "                    labels = (batch[\"nutrition\"] - global_mean) / global_std\n",
    "                else:\n",
    "                    # If nutrition is a list or dictionary, adjust accordingly\n",
    "                    labels = process_labels(batch[\"nutrition\"], global_mean, global_std, device)\n",
    "                labels = labels.float().to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                pred = model(activity_data, cgm_data, demographics).squeeze(1)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(pred, labels)\n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            validation_losses.append(avg_val_loss)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss\n",
    "            }, 'best_model.pth')\n",
    "            print(f\"Saved new best model with validation loss: {avg_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        # Optional: print current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current learning rate: {current_lr}\")\n",
    "    \n",
    "    print(\"Training Complete!\")\n",
    "    return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UnifiedLSTMModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 activity_channels=2,\n",
    "                 cgm_channels=1, \n",
    "                 seq_length=1440,\n",
    "                 hidden_size=128,\n",
    "                 num_layers=2,\n",
    "                 dropout=0.2,\n",
    "                 demographic_features=5,\n",
    "                 output_size=1):\n",
    "        super(UnifiedLSTMModel, self).__init__()\n",
    "        \n",
    "        # Total number of input channels/features\n",
    "        self.total_channels = activity_channels + cgm_channels\n",
    "        \n",
    "        # Main LSTM for processing all time series data\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.total_channels,  # All channels combined\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism (optional but helpful)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size * 2,  # * 2 for bidirectional\n",
    "            num_heads=4,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers for final prediction\n",
    "        self.fc1 = nn.Linear(hidden_size * 2 + demographic_features, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Batch normalization for improved training stability\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "    \n",
    "    def forward(self, activity_data, cgm_data, demographics):\n",
    "        batch_size = activity_data.size(0)\n",
    "        \n",
    "        # Prepare and combine all time series data\n",
    "        # Reshape inputs to ensure consistent dimensions\n",
    "        if len(cgm_data.shape) == 3:  # If already [batch, channels, seq]\n",
    "            cgm_data = cgm_data.permute(0, 2, 1)  # -> [batch, seq, channels]\n",
    "        elif len(cgm_data.shape) == 2:  # If [batch, seq]\n",
    "            cgm_data = cgm_data.unsqueeze(-1)  # -> [batch, seq, 1]\n",
    "            \n",
    "        activity_data = activity_data.permute(0, 2, 1)  # [batch, seq, channels]\n",
    "            \n",
    "        # Concatenate all time series features along the channel dimension\n",
    "        combined_data = torch.cat([activity_data, cgm_data], dim=2)\n",
    "        \n",
    "        # Process through LSTM\n",
    "        lstm_out, (hidden, _) = self.lstm(combined_data)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        lstm_out = lstm_out.permute(1, 0, 2)  # [seq, batch, features]\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        attn_out = attn_out.permute(1, 0, 2)  # [batch, seq, features]\n",
    "        \n",
    "        # Get context vector (last hidden state)\n",
    "        context = attn_out[:, -1, :]  # Use the last time step\n",
    "        context = self.bn1(context)  # Apply batch normalization\n",
    "        \n",
    "        # Concatenate with demographics\n",
    "        combined_features = torch.cat([context, demographics], dim=1)\n",
    "        \n",
    "        # Final prediction layers\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc2(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1637384/559066402.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
      "Training Epochs:   0%|          | 0/30 [00:00<?, ?it/s]/tmp/ipykernel_1637384/559066402.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions: tensor([ 0.5942, -0.4089, -0.2798,  0.1531, -0.4253], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SliceBackward0>)\n",
      "Sample labels: tensor([-1.8267,  0.9100, -2.4767, -2.4433, -1.4633], device='cuda:0')\n",
      "Initial loss value: 3.225468635559082\n",
      "Epoch [1/30], Training Loss: 3.6601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   3%|▎         | 1/30 [00:03<01:50,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Validation Loss: 3.9155\n",
      "Saved new best model with validation loss: 3.9155\n",
      "Current learning rate: 0.001\n",
      "Epoch [2/30], Training Loss: 3.3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   7%|▋         | 2/30 [00:07<01:46,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], Validation Loss: 3.7399\n",
      "Saved new best model with validation loss: 3.7399\n",
      "Current learning rate: 0.001\n",
      "Epoch [3/30], Training Loss: 3.3834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|█         | 3/30 [00:11<01:42,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], Validation Loss: 3.7136\n",
      "Saved new best model with validation loss: 3.7136\n",
      "Current learning rate: 0.001\n",
      "Epoch [4/30], Training Loss: 3.4337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  13%|█▎        | 4/30 [00:15<01:37,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30], Validation Loss: 3.7553\n",
      "Current learning rate: 0.001\n",
      "Epoch [5/30], Training Loss: 3.2725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  17%|█▋        | 5/30 [00:18<01:33,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Validation Loss: 3.7185\n",
      "Current learning rate: 0.001\n",
      "Epoch [6/30], Training Loss: 3.3886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|██        | 6/30 [00:22<01:29,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30], Validation Loss: 4.6449\n",
      "Current learning rate: 0.001\n",
      "Epoch [7/30], Training Loss: 3.3597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  23%|██▎       | 7/30 [00:26<01:26,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], Validation Loss: 3.6103\n",
      "Saved new best model with validation loss: 3.6103\n",
      "Current learning rate: 0.001\n",
      "Epoch [8/30], Training Loss: 3.1899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  27%|██▋       | 8/30 [00:30<01:22,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30], Validation Loss: 3.5842\n",
      "Saved new best model with validation loss: 3.5842\n",
      "Current learning rate: 0.001\n",
      "Epoch [9/30], Training Loss: 3.2932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  30%|███       | 9/30 [00:33<01:18,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30], Validation Loss: 3.6605\n",
      "Current learning rate: 0.001\n",
      "Epoch [10/30], Training Loss: 3.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  33%|███▎      | 10/30 [00:37<01:15,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Validation Loss: 3.9158\n",
      "Current learning rate: 0.001\n",
      "Epoch [11/30], Training Loss: 3.1931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  37%|███▋      | 11/30 [00:41<01:11,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], Validation Loss: 3.6249\n",
      "Current learning rate: 0.001\n",
      "Epoch [12/30], Training Loss: 3.2860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|████      | 12/30 [00:45<01:07,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30], Validation Loss: 3.7385\n",
      "Current learning rate: 0.0005\n",
      "Epoch [13/30], Training Loss: 3.2548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  43%|████▎     | 13/30 [00:48<01:03,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30], Validation Loss: 4.1433\n",
      "Current learning rate: 0.0005\n",
      "Epoch [14/30], Training Loss: 3.3010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  47%|████▋     | 14/30 [00:52<00:59,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30], Validation Loss: 3.6314\n",
      "Current learning rate: 0.0005\n",
      "Epoch [15/30], Training Loss: 3.1972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  50%|█████     | 15/30 [00:56<00:56,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Validation Loss: 3.4937\n",
      "Saved new best model with validation loss: 3.4937\n",
      "Current learning rate: 0.0005\n",
      "Epoch [16/30], Training Loss: 2.9662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  53%|█████▎    | 16/30 [01:00<00:52,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30], Validation Loss: 3.3825\n",
      "Saved new best model with validation loss: 3.3825\n",
      "Current learning rate: 0.0005\n",
      "Epoch [17/30], Training Loss: 3.0412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  57%|█████▋    | 17/30 [01:03<00:48,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30], Validation Loss: 3.5062\n",
      "Current learning rate: 0.0005\n",
      "Epoch [18/30], Training Loss: 3.0758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  60%|██████    | 18/30 [01:07<00:44,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/30], Validation Loss: 3.5137\n",
      "Current learning rate: 0.0005\n",
      "Epoch [19/30], Training Loss: 2.9310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  63%|██████▎   | 19/30 [01:11<00:41,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30], Validation Loss: 3.4194\n",
      "Current learning rate: 0.0005\n",
      "Epoch [20/30], Training Loss: 2.8319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  67%|██████▋   | 20/30 [01:15<00:37,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Validation Loss: 3.9283\n",
      "Current learning rate: 0.00025\n",
      "Epoch [21/30], Training Loss: 2.7665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  70%|███████   | 21/30 [01:18<00:33,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], Validation Loss: 3.3480\n",
      "Saved new best model with validation loss: 3.3480\n",
      "Current learning rate: 0.00025\n",
      "Epoch [22/30], Training Loss: 2.8383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  73%|███████▎  | 22/30 [01:22<00:30,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30], Validation Loss: 3.4448\n",
      "Current learning rate: 0.00025\n",
      "Epoch [23/30], Training Loss: 2.8035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  77%|███████▋  | 23/30 [01:26<00:26,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/30], Validation Loss: 3.4165\n",
      "Current learning rate: 0.00025\n",
      "Epoch [24/30], Training Loss: 2.7234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  80%|████████  | 24/30 [01:29<00:22,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/30], Validation Loss: 3.6152\n",
      "Current learning rate: 0.00025\n",
      "Epoch [25/30], Training Loss: 2.7277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  83%|████████▎ | 25/30 [01:33<00:18,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30], Validation Loss: 3.4644\n",
      "Current learning rate: 0.000125\n",
      "Epoch [26/30], Training Loss: 2.8118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  87%|████████▋ | 26/30 [01:37<00:14,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30], Validation Loss: 3.3572\n",
      "Current learning rate: 0.000125\n",
      "Epoch [27/30], Training Loss: 2.8048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  90%|█████████ | 27/30 [01:41<00:11,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/30], Validation Loss: 3.3683\n",
      "Current learning rate: 0.000125\n",
      "Epoch [28/30], Training Loss: 2.7654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  90%|█████████ | 27/30 [01:44<00:11,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30], Validation Loss: 3.4262\n",
      "Early stopping triggered after 28 epochs\n",
      "Training Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the unified model (without meal time features)\n",
    "unified_model = UnifiedLSTMModel(\n",
    "    activity_channels=2,\n",
    "    cgm_channels=1,\n",
    "    seq_length=1440,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    "    demographic_features=5,\n",
    "    output_size=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "training_losses, validation_losses = train_unified_model(\n",
    "    unified_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    global_mean=2000,\n",
    "    global_std=300,\n",
    "    epochs=30,\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgm_dinner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
