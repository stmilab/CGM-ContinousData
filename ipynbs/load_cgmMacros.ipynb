{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_time_series_data(df, cgm_cols=[\"Dexcom GL\", \"Libre GL\"], activity_cols=[\"HR\", \"METs\"]):\n",
    "    \"\"\"Clean and validate time-series data\"\"\"\n",
    "    # Copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Handle missing values\n",
    "    for col in cgm_cols + activity_cols:\n",
    "        if col in df.columns:\n",
    "            # Linear interpolation with 5-minute window\n",
    "            df[col] = df[col].interpolate(method='linear', limit=5)\n",
    "            # Forward/backward fill remaining\n",
    "            df[col] = df[col].ffill().bfill()\n",
    "    \n",
    "    # 2. Remove physiologically impossible values\n",
    "    if \"Dexcom GL\" in df.columns:\n",
    "        df = df[(df[\"Dexcom GL\"] >= 40) & (df[\"Dexcom GL\"] <= 400)]\n",
    "    if \"HR\" in df.columns:\n",
    "        df = df[(df[\"HR\"] >= 40) & (df[\"HR\"] <= 200)]\n",
    "    if \"METs\" in df.columns:\n",
    "        df = df[(df[\"METs\"] >= 0.5) & (df[\"METs\"] <= 20)]\n",
    "    \n",
    "    # 3. Smooth noisy data (Savitzky-Golay filter)\n",
    "    for col in cgm_cols + activity_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                from scipy.signal import savgol_filter\n",
    "                df[col] = savgol_filter(df[col], window_length=15, polyorder=2)\n",
    "            except ImportError:\n",
    "                # Fallback to rolling mean if scipy not available\n",
    "                df[col] = df[col].rolling(window=15, min_periods=1, center=True).mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, pdb, copy\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_image(\n",
    "    img_filename: str,\n",
    "    subject_id: int,\n",
    "    target_size: tuple,\n",
    "    cgmacros_path: str = \"CGMacros 2/\",\n",
    ") -> np.ndarray:\n",
    "    subject_path = f\"CGMacros-{subject_id:03d}/\"\n",
    "    img_path = f\"{cgmacros_path}{subject_path}{img_filename}\"\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"File {img_path} not found\")\n",
    "        raise FileNotFoundError\n",
    "    # Loading names out\n",
    "    img_data = cv2.resize(\n",
    "        cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB),\n",
    "        target_size,\n",
    "        interpolation=cv2.INTER_LANCZOS4,\n",
    "    )\n",
    "    return img_data\n",
    "\n",
    "\n",
    "def load_CGMacros(\n",
    "    subject_id: int,\n",
    "    csv_dir: str = \"CGMacros 2\",\n",
    ") -> pd.DataFrame:\n",
    "    if type(subject_id) != int:\n",
    "        print(\"subject_id should be an integer\")\n",
    "        raise ValueError\n",
    "    subejct_path = f\"CGMacros-{subject_id:03d}/CGMacros-{subject_id:03d}.csv\"\n",
    "    subject_file = os.path.join(csv_dir, subejct_path)\n",
    "    if not os.path.exists(subject_file):\n",
    "        tqdm.write(f\"File {subject_file} not found\")\n",
    "        raise FileNotFoundError\n",
    "    dataset_df = pd.read_csv(subject_file, index_col=None)\n",
    "    dataset_df[\"Timestamp\"] = pd.to_datetime(dataset_df[\"Timestamp\"])\n",
    "    dataset_df = clean_time_series_data(dataset_df)  # Add cleaning step\n",
    "    return dataset_df.set_index(\"Timestamp\")\n",
    "\n",
    "def load_daily_traces(\n",
    "    dataset_df: pd.DataFrame,\n",
    "    subject_id: int,\n",
    "    cgm_cols=[\"Dexcom GL\", \"Libre GL\"],\n",
    "    activity_cols=[\"HR\", \"Calories (Activity)\"],\n",
    "    img_size=(112, 112)\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced version with data cleaning, normalization, and NaN handling using median for time-series data (CGM and activity).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (\n",
    "            days_list: List of date strings,\n",
    "            cgm_daily_data: Normalized array of shape (n_days, n_cgm_cols, 1440),\n",
    "            activity_daily_data: Normalized array of shape (n_days, n_activity_cols, 1440),\n",
    "            image_data_by_day: dict of image data,\n",
    "            nutrition_data_by_day: dict of nutrition data,\n",
    "            cgm_stats: dict of normalization parameters {'mean', 'std'},\n",
    "            activity_stats: dict of normalization parameters {'mean', 'std'}\n",
    "        )\n",
    "    \"\"\"\n",
    "    # 1. Clean and preprocess the raw data\n",
    "    def clean_series(series):\n",
    "        \"\"\"Helper function to clean a time series\"\"\"\n",
    "        # Interpolation\n",
    "        series = series.interpolate(method='linear', limit=5).ffill().bfill()\n",
    "        \n",
    "        # Smoothing (Savitzky-Golay if available, else rolling mean)\n",
    "        try:\n",
    "            from scipy.signal import savgol_filter\n",
    "            series = savgol_filter(series, window_length=15, polyorder=2)\n",
    "        except ImportError:\n",
    "            series = series.rolling(window=15, min_periods=1, center=True).mean()\n",
    "        \n",
    "        return series\n",
    "\n",
    "    # 2. Apply cleaning to each relevant column (CGM and activity)\n",
    "    cleaned_df = dataset_df.copy()\n",
    "    for col in cgm_cols + activity_cols:\n",
    "        if col in cleaned_df.columns:\n",
    "            cleaned_df[col] = clean_series(cleaned_df[col])\n",
    "    \n",
    "    # 3. Resample to 1-minute frequency and handle NaNs after resampling with median\n",
    "    resampled_df = cleaned_df.resample('1min').ffill(limit=5)\n",
    "    \n",
    "    # Fill NaNs with the median of each column (for both CGM and activity data)\n",
    "    for col in cgm_cols + activity_cols:\n",
    "        resampled_df[col] = resampled_df[col].fillna(resampled_df[col].median())\n",
    "    \n",
    "    # 4. Calculate normalization statistics\n",
    "    cgm_stats = {\n",
    "        'mean': resampled_df[cgm_cols].mean().values,\n",
    "        'std': resampled_df[cgm_cols].std().values\n",
    "    }\n",
    "    activity_stats = {\n",
    "        'mean': resampled_df[activity_cols].mean().values,\n",
    "        'std': resampled_df[activity_cols].std().values\n",
    "    }\n",
    "\n",
    "    # 5. Initialize arrays for CGM and activity data\n",
    "    days = pd.Series(resampled_df.index.date).unique()\n",
    "    days_list = [str(day) for day in days]\n",
    "    cgm_daily_data = np.full((len(days), len(cgm_cols), 1440), np.nan)\n",
    "    activity_daily_data = np.full((len(days), len(activity_cols), 1440), np.nan)\n",
    "    image_data_by_day = {}\n",
    "    nutrition_data_by_day = {}\n",
    "\n",
    "    # 6. Process each day\n",
    "    for i, day in enumerate(days):\n",
    "        day_start = pd.Timestamp(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1) - pd.Timedelta(minutes=1)\n",
    "        day_data = resampled_df.loc[day_start:day_end]\n",
    "        \n",
    "        if not day_data.empty:\n",
    "            minutes_of_day = (day_data.index.hour * 60 + day_data.index.minute).values\n",
    "            \n",
    "            # Process and normalize CGM data\n",
    "            for j, col in enumerate(cgm_cols):\n",
    "                if col in day_data.columns:\n",
    "                    vals = day_data[col].values\n",
    "                    # Replace NaN values with the median of the column for that day\n",
    "                    vals = np.nan_to_num(vals, nan=day_data[col].median())\n",
    "                    # Normalize the values\n",
    "                    vals = (vals - cgm_stats['mean'][j]) / (cgm_stats['std'][j] + 1e-8)\n",
    "                    cgm_daily_data[i, j, minutes_of_day] = vals\n",
    "            \n",
    "            # Process and normalize activity data\n",
    "            for j, col in enumerate(activity_cols):\n",
    "                if col in day_data.columns:\n",
    "                    vals = day_data[col].values\n",
    "                    # Replace NaN values with the median of the column for that day\n",
    "                    vals = np.nan_to_num(vals, nan=day_data[col].median())\n",
    "                    # Normalize the values\n",
    "                    vals = (vals - activity_stats['mean'][j]) / (activity_stats['std'][j] + 1e-8)\n",
    "                    activity_daily_data[i, j, minutes_of_day] = vals\n",
    "        \n",
    "        # Process nutrition and image data (with NaN handling)\n",
    "        day_str = str(day)\n",
    "        original_day_data = dataset_df.loc[day_start:day_end]\n",
    "        \n",
    "        # Nutrition data processing...\n",
    "        nutrition_rows = original_day_data.dropna(subset=['Calories', 'Carbs', 'Protein', 'Fat', 'Fiber'], how='all')\n",
    "        day_nutrition = []\n",
    "        for ts, row in nutrition_rows.iterrows():\n",
    "            nutrition = {\n",
    "                'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'MealType': row['Meal Type'],\n",
    "                'calories': row['Calories'] if pd.notna(row['Calories']) else 0,\n",
    "                'carbs': row['Carbs'] if pd.notna(row['Carbs']) else 0,\n",
    "                'protein': row['Protein'] if pd.notna(row['Protein']) else 0,\n",
    "                'fat': row['Fat'] if pd.notna(row['Fat']) else 0,\n",
    "                'fiber': row['Fiber'] if pd.notna(row['Fiber']) else 0,\n",
    "                'has_image': pd.notna(row['Image path'])\n",
    "            }\n",
    "            day_nutrition.append(nutrition)\n",
    "        nutrition_data_by_day[day_str] = day_nutrition\n",
    "        \n",
    "        # Image data processing (with NaN handling for missing paths)\n",
    "        image_rows = original_day_data.dropna(subset=['Image path'])\n",
    "        day_images = []\n",
    "        for ts, row in image_rows.iterrows():\n",
    "            try:\n",
    "                img_data = get_image(row['Image path'], subject_id, img_size)\n",
    "                metadata = {\n",
    "                    'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'meal_type': row['Meal Type'] if 'Meal Type' in row else None,\n",
    "                    'calories': row['Calories'] if 'Calories' in row else None,\n",
    "                    'carbs': row['Carbs'] if 'Carbs' in row else None,\n",
    "                    'protein': row['Protein'] if 'Protein' in row else None,\n",
    "                    'fat': row['Fat'] if 'Fat' in row else None,\n",
    "                    'fiber': row['Fiber'] if 'Fiber' in row else None\n",
    "                }\n",
    "                day_images.append({'image': img_data, 'metadata': metadata})\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        image_data_by_day[day_str] = day_images if day_images else []\n",
    "\n",
    "    return (\n",
    "        days_list,\n",
    "        cgm_daily_data,          # Now normalized\n",
    "        activity_daily_data,     # Now normalized\n",
    "        image_data_by_day,\n",
    "        nutrition_data_by_day,\n",
    "        cgm_stats,               # New: normalization parameters\n",
    "        activity_stats           # New: normalization parameters\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def create_daily_dataset(\n",
    "    subject_id: int,\n",
    "    csv_dir: str = \"CGMacros 2\",\n",
    "    cgm_cols=[\"Dexcom GL\", \"Libre GL\"],\n",
    "    activity_cols=[\"HR\"],  # Consider alternative spellings\n",
    "    img_size=(112, 112),\n",
    "    verbose=False\n",
    "):\n",
    "    try:\n",
    "        # 1. Load data with column validation\n",
    "        dataset_df = load_CGMacros(subject_id, csv_dir)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Available columns:\", dataset_df.columns.tolist())\n",
    "        \n",
    "        # 2. Handle missing columns gracefully\n",
    "        available_activity_cols = [col for col in activity_cols \n",
    "                                 if col in dataset_df.columns]\n",
    "        if len(available_activity_cols) < len(activity_cols):\n",
    "            print(f\"Warning: Missing activity columns. Using {available_activity_cols} for subject {subject_id}\")\n",
    "        \n",
    "        # 3. Process data with validated columns\n",
    "        result = load_daily_traces(\n",
    "            dataset_df, subject_id, \n",
    "            cgm_cols=cgm_cols,\n",
    "            activity_cols=available_activity_cols,  # Use only available columns\n",
    "            img_size=img_size\n",
    "        )\n",
    "        \n",
    "        return (subject_id,) + result[:5]  # Return first 5 elements + subject_id\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Data for subject {subject_id} not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject {subject_id}: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def process_multiple_subjects(\n",
    "    subject_ids=None,\n",
    "    csv_dir=\"CGMacros 2\",\n",
    "    save_dir=\"processed_data/\",\n",
    "    cgm_cols=[\"Dexcom GL\",\"Libre GL\"],\n",
    "    activity_cols=[\"HR\"],\n",
    "    img_size=(112, 112)\n",
    "):\n",
    "    \"\"\"\n",
    "    Process multiple subjects and save their daily trace data.\n",
    "    \n",
    "    Args:\n",
    "        subject_ids (list): List of subject IDs to process. If None, try subjects 1-50.\n",
    "        csv_dir (str): Directory containing the CGMacros data\n",
    "        save_dir (str): Directory to save processed data\n",
    "        cgm_cols (list): List of CGM columns to extract\n",
    "        activity_cols (list): List of activity columns to extract\n",
    "        img_size (tuple): Size to resize images to\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary of processed data\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    if subject_ids is None:\n",
    "        subject_ids = range(1, 51)  # Try subjects 1-50\n",
    "    \n",
    "    summary = {\n",
    "        'processed_subjects': [],\n",
    "        'total_days': 0,\n",
    "        'total_images': 0,\n",
    "        'total_meals':0\n",
    "    }\n",
    "    \n",
    "    for subject_id in tqdm(subject_ids, desc=\"Processing subjects\"):\n",
    "        result = create_daily_dataset(subject_id, csv_dir)\n",
    "        if result is None:\n",
    "            continue\n",
    "            \n",
    "        subject_id, days, cgm, activity, images, nutrition = result  # Unpack new return\n",
    "        \n",
    "        # Save data\n",
    "        subject_data = {\n",
    "            'subject_id': subject_id,\n",
    "            'days': days,\n",
    "            'cgm_data': cgm,\n",
    "            'activity_data': activity,\n",
    "            'image_data': images,\n",
    "            'nutrition_data': nutrition  # NEW: Store nutrition separately\n",
    "        }\n",
    "        torch.save(subject_data, os.path.join(save_dir, f\"subject_{subject_id:03d}_daily_data.pt\"))\n",
    "        \n",
    "        # Update summary counts (can add nutrition-specific metrics)\n",
    "        summary['processed_subjects'].append(subject_id)\n",
    "        summary['total_days'] += len(days)\n",
    "        summary['total_images'] += sum(len(imgs) for imgs in images.values())\n",
    "        summary['total_meals'] += sum(len(meals) for meals in nutrition.values())\n",
    "    return summary\n",
    "\n",
    "class DailyTracesDataset(Dataset):\n",
    "    def __init__(self, data_dir, subject_ids=None, transform=None, skip_days=[1]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Directory containing processed .pt files\n",
    "            subject_ids (list): Optional list of subject IDs to include\n",
    "            transform (callable): Optional transform for images/time-series\n",
    "            skip_days (list): Day numbers to exclude (e.g., [1] skips day 1 of each month)\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.skip_days = skip_days or [1]  # Default to no skipped days\n",
    "        \n",
    "        # Find relevant subject files\n",
    "        if subject_ids is None:\n",
    "            self.data_files = [\n",
    "                f for f in os.listdir(data_dir) \n",
    "                if f.startswith(\"subject_\") and f.endswith(\"_daily_data.pt\")\n",
    "            ]\n",
    "        else:\n",
    "            self.data_files = [\n",
    "                f\"subject_{sid:03d}_daily_data.pt\" \n",
    "                for sid in subject_ids \n",
    "                if os.path.exists(os.path.join(data_dir, f\"subject_{sid:03d}_daily_data.pt\"))\n",
    "            ]\n",
    "        \n",
    "        # Build indices accounting for skip_days\n",
    "        self.indices = []\n",
    "        self.subject_day_pairs = []\n",
    "        \n",
    "        for file_idx, fname in enumerate(self.data_files):\n",
    "            data = torch.load(os.path.join(data_dir, fname))\n",
    "            subject_id = data['subject_id']\n",
    "            \n",
    "            for day_idx, day_str in enumerate(data['days']):\n",
    "                day_num = int(day_str.split('-')[2])\n",
    "                if day_num not in self.skip_days:\n",
    "                    self.indices.append((file_idx, day_idx))\n",
    "                    self.subject_day_pairs.append((subject_id, day_num))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns total number of valid (subject, day) pairs after filtering\"\"\"\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, day_idx = self.indices[idx]\n",
    "        data = torch.load(os.path.join(self.data_dir, self.data_files[file_idx]))\n",
    "        day = data['days'][day_idx]\n",
    "        \n",
    "        # Apply transforms if specified\n",
    "        def _apply_transform(x):\n",
    "            return self.transform(x) if self.transform else x\n",
    "        \n",
    "        return {\n",
    "            'subject_id': data['subject_id'],\n",
    "            'day': day,\n",
    "            'cgm_data': _apply_transform(data['cgm_data'][day_idx]),\n",
    "            'activity_data': _apply_transform(data['activity_data'][day_idx]),\n",
    "            'images': [_apply_transform(img['image']) for img in data['image_data'].get(day, [])],\n",
    "            'nutrition': data['nutrition_data'].get(day, []),\n",
    "            'subject_day_pair': self.subject_day_pairs[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "def split_dataset_by_subject_day(dataset, test_size=0.2, random_state=2025):\n",
    "    \"\"\"\n",
    "    Split the dataset based on subject-day pairs to ensure all data from\n",
    "    the same subject and day stays together in either training or testing set.\n",
    "    \n",
    "    Args:\n",
    "        dataset (DailyTracesDataset): The dataset to split\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_indices, test_indices)\n",
    "    \"\"\"\n",
    "    # Get unique subject-day pairs\n",
    "    subject_day_df = pd.DataFrame(dataset.subject_day_pairs, columns=['subject_id', 'day_id'])\n",
    "    unique_pairs = subject_day_df.drop_duplicates()\n",
    "    \n",
    "    # Split the unique subject-day pairs\n",
    "    train_pairs, test_pairs = train_test_split(\n",
    "        unique_pairs, \n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Convert to sets for faster lookup\n",
    "    train_pairs_set = set(zip(train_pairs['subject_id'], train_pairs['day_id']))\n",
    "    test_pairs_set = set(zip(test_pairs['subject_id'], test_pairs['day_id']))\n",
    "    \n",
    "    # Create masks for train and test indices\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    for i, (subject_id, day_id) in enumerate(dataset.subject_day_pairs):\n",
    "        if (subject_id, day_id) in train_pairs_set:\n",
    "            train_indices.append(i)\n",
    "        elif (subject_id, day_id) in test_pairs_set:\n",
    "            test_indices.append(i)\n",
    "    \n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "class SubjectDaySubset(Dataset):\n",
    "    \"\"\"\n",
    "    Subset of DailyTracesDataset based on indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "\n",
    "def get_train_test_datasets(data_dir, subject_ids=None, test_size=0.2, random_state=2025, transform=None):\n",
    "    \"\"\"\n",
    "    Get train and test datasets split by subject-day pairs.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Directory containing processed data\n",
    "        subject_ids (list): List of subject IDs to include. If None, include all available.\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        random_state (int): Random seed for reproducibility\n",
    "        transform (callable): Optional transform to apply to the data\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_dataset, test_dataset)\n",
    "    \"\"\"\n",
    "    # Create the full dataset\n",
    "    full_dataset = DailyTracesDataset(data_dir, subject_ids, transform)\n",
    "    \n",
    "    # Split by subject-day pairs\n",
    "    train_indices, test_indices = split_dataset_by_subject_day(full_dataset, test_size, random_state)\n",
    "    \n",
    "    # Create train and test subsets\n",
    "    train_dataset = SubjectDaySubset(full_dataset, train_indices)\n",
    "    test_dataset = SubjectDaySubset(full_dataset, test_indices)\n",
    "    \n",
    "    print(f\"Full dataset size: {len(full_dataset)}\")\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def extract_image_features(images, feature_extractor):\n",
    "    \"\"\"\n",
    "    Extract features from a list of images using a pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "        images (list): List of image arrays\n",
    "        feature_extractor: Model for feature extraction\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of image features\n",
    "    \"\"\"\n",
    "    if not images:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Stack images into a batch\n",
    "    batch = np.stack([img_data['image'] for img_data in images])\n",
    "    batch_tensor = torch.tensor(batch).float().permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
    "    \n",
    "    # Normalize if needed\n",
    "    if batch_tensor.max() > 1.0:\n",
    "        batch_tensor = batch_tensor / 255.0\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(batch_tensor)\n",
    "    \n",
    "    return features.cpu().numpy()\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \"\"\"Handles variable-length nutrition data and images\"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def fix_nans(array):\n",
    "        \"\"\"Replace NaNs with median (per channel)\"\"\"\n",
    "        median_vals = np.nanmedian(array, axis=1, keepdims=True)  # Compute median per channel\n",
    "        return np.where(np.isnan(array), median_vals, array)  # Replace NaNs\n",
    "\n",
    "    # Fix NaNs before converting to tensors\n",
    "    for i, x in enumerate(batch):\n",
    "        x['cgm_data'] = fix_nans(x['cgm_data'])\n",
    "        x['activity_data'] = fix_nans(x['activity_data'])\n",
    "\n",
    "    return {\n",
    "        'subject_ids': torch.tensor([x['subject_id'] for x in batch]),\n",
    "        'days': [x['day'] for x in batch],\n",
    "        'cgm_data': torch.stack([torch.tensor(x['cgm_data'], dtype=torch.float32) for x in batch]),\n",
    "        'activity_data': torch.stack([torch.tensor(x['activity_data'], dtype=torch.float32) for x in batch]),\n",
    "        'images': [x['images'] for x in batch],  # List of lists (variable length)\n",
    "        'nutrition': [x['nutrition'] for x in batch],  # List of lists\n",
    "        'subject_day_pairs': [x['subject_day_pair'] for x in batch]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def main():\n",
    "    # Step 1: Process the raw CSV data for subjects and save the daily traces\n",
    "    # You can adjust the subject IDs, directories, etc. as needed\n",
    "    summary = process_multiple_subjects(\n",
    "        subject_ids=range(1, 50),  # Process subjects 1-10\n",
    "        csv_dir=\"CGMacros 2\",  # Path to your CSV files\n",
    "        save_dir=\"processed_data/\",    # Where to save processed data\n",
    "        cgm_cols=[\"Dexcom GL\",\"Libre GL\"],\n",
    "        activity_cols=[\"HR\"],\n",
    "        img_size=(112, 112)\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing summary:\")\n",
    "    print(f\"- Processed {len(summary['processed_subjects'])} subjects\")\n",
    "    print(f\"- Total days: {summary['total_days']}\")\n",
    "    print(f\"- Total images: {summary['total_images']}\")\n",
    "    \n",
    "    # Step 2: Create train and test datasets from the processed data\n",
    "    train_dataset, test_dataset = get_train_test_datasets(\n",
    "        data_dir=\"processed_data/\",\n",
    "        subject_ids=None,  # Use all available subjects\n",
    "        test_size=0.2,     # 80% train, 20% test\n",
    "        random_state=2025, # For reproducibility\n",
    "        transform=None     # Add any transforms you need\n",
    "    )\n",
    "    \n",
    "    print(\"Done creating train and test datasets\")\n",
    "\n",
    "    # Step 3: Create DataLoaders for efficient batching\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=custom_collate\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=custom_collate\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:   2%|▏         | 1/49 [00:00<00:24,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros 2/CGMacros-001/photos/00000075-PHOTO-2020-5-9-9-48-0.jpg not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:   4%|▍         | 2/49 [00:01<00:23,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros 2/CGMacros-002/photos/00000083-PHOTO-2019-11-24-12-13-0.jpg not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:  47%|████▋     | 23/49 [00:10<00:12,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros 2/CGMacros-024/CGMacros-024.csv not found\n",
      "Data for subject 24 not found.\n",
      "File CGMacros 2/CGMacros-025/CGMacros-025.csv not found\n",
      "Data for subject 25 not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:  73%|███████▎  | 36/49 [00:16<00:08,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros 2/CGMacros-037/CGMacros-037.csv not found\n",
      "Data for subject 37 not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:  80%|███████▉  | 39/49 [00:17<00:04,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros 2/CGMacros-040/CGMacros-040.csv not found\n",
      "Data for subject 40 not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects: 100%|██████████| 49/49 [00:23<00:00,  2.10it/s]\n",
      "/var/folders/ch/llzrb2pd6hgf5s9k02bw3cq40000gn/T/ipykernel_17028/1158003765.py:327: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(os.path.join(data_dir, fname))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing summary:\n",
      "- Processed 45 subjects\n",
      "- Total days: 532\n",
      "- Total images: 2289\n",
      "Full dataset size: 511\n",
      "Train dataset size: 408\n",
      "Test dataset size: 103\n",
      "Done creating train and test datasets\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:   2%|▏         | 1/49 [00:00<00:24,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros 2/CGMacros-001/photos/00000075-PHOTO-2020-5-9-9-48-0.jpg not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:   4%|▍         | 2/49 [00:01<00:24,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros 2/CGMacros-002/photos/00000083-PHOTO-2019-11-24-12-13-0.jpg not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:  47%|████▋     | 23/49 [00:10<00:12,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros 2/CGMacros-024/CGMacros-024.csv not found\n",
      "Data for subject 24 not found.\n",
      "File CGMacros 2/CGMacros-025/CGMacros-025.csv not found\n",
      "Data for subject 25 not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:  73%|███████▎  | 36/49 [00:16<00:07,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros 2/CGMacros-037/CGMacros-037.csv not found\n",
      "Data for subject 37 not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:  80%|███████▉  | 39/49 [00:17<00:04,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros 2/CGMacros-040/CGMacros-040.csv not found\n",
      "Data for subject 40 not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects: 100%|██████████| 49/49 [00:22<00:00,  2.22it/s]\n",
      "/var/folders/ch/llzrb2pd6hgf5s9k02bw3cq40000gn/T/ipykernel_17028/1158003765.py:327: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(os.path.join(data_dir, fname))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing summary:\n",
      "- Processed 45 subjects\n",
      "- Total days: 532\n",
      "- Total images: 2289\n",
      "Full dataset size: 511\n",
      "Train dataset size: 408\n",
      "Test dataset size: 103\n"
     ]
    }
   ],
   "source": [
    "summary = process_multiple_subjects(\n",
    "    subject_ids=range(1, 50),  # Process subjects 1-10\n",
    "    csv_dir=\"CGMacros 2\",  # Path to your CSV files\n",
    "    save_dir=\"processed_data/\",    # Where to save processed data\n",
    "    cgm_cols=[\"Dexcom GL\",\"Libre GL\"],\n",
    "    activity_cols=[\"HR\"],\n",
    "    img_size=(112, 112)\n",
    ")\n",
    "\n",
    "print(f\"Processing summary:\")\n",
    "print(f\"- Processed {len(summary['processed_subjects'])} subjects\")\n",
    "print(f\"- Total days: {summary['total_days']}\")\n",
    "print(f\"- Total images: {summary['total_images']}\")\n",
    "\n",
    "# Step 2: Create train and test datasets from the processed data\n",
    "train_dataset, test_dataset = get_train_test_datasets(\n",
    "    data_dir=\"processed_data/\",\n",
    "    subject_ids=None,  # Use all available subjects\n",
    "    test_size=0.2,     # 80% train, 20% test\n",
    "    random_state=2025, # For reproducibility\n",
    "    transform=None     # Add any transforms you need\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/llzrb2pd6hgf5s9k02bw3cq40000gn/T/ipykernel_17028/1158003765.py:342: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(os.path.join(self.data_dir, self.data_files[file_idx]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN handling completed. No NaNs remain in CGM and Activity data.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for sample in test_dataset:\n",
    "    # Fix NaNs in CGM Data\n",
    "    if 'cgm_data' in sample and sample['cgm_data'] is not None:\n",
    "        cgm_data = sample['cgm_data']\n",
    "        for i in range(cgm_data.shape[0]):  # Iterate over each row (sensor type)\n",
    "            nan_mask = np.isnan(cgm_data[i])  # Identify NaNs\n",
    "            if np.any(nan_mask):  # If NaNs exist, replace with median\n",
    "                median_val = np.nanmedian(cgm_data[i])  # Compute median ignoring NaNs\n",
    "                cgm_data[i, nan_mask] = median_val\n",
    "\n",
    "    # Fix NaNs in Activity Data\n",
    "    if 'activity_data' in sample and sample['activity_data'] is not None:\n",
    "        activity_data = sample['activity_data']\n",
    "        for i in range(activity_data.shape[0]):  # Iterate over each row (sensor type)\n",
    "            nan_mask = np.isnan(activity_data[i])  # Identify NaNs\n",
    "            if np.any(nan_mask):  # If NaNs exist, replace with median\n",
    "                median_val = np.nanmedian(activity_data[i])  # Compute median ignoring NaNs\n",
    "                activity_data[i, nan_mask] = median_val\n",
    "\n",
    "# Confirm all NaNs have been replaced\n",
    "assert not np.any(np.isnan(train_dataset[0]['cgm_data'])), \"NaNs still exist in CGM data!\"\n",
    "assert not np.any(np.isnan(train_dataset[0]['activity_data'])), \"NaNs still exist in Activity data!\"\n",
    "\n",
    "print(\"NaN handling completed. No NaNs remain in CGM and Activity data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate2(batch):\n",
    "    \"\"\"Handles variable-length nutrition data and images\"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def fix_nans(array):\n",
    "        \"\"\"Replace NaNs with median (per channel)\"\"\"\n",
    "        median_vals = np.nanmedian(array, axis=1, keepdims=True)  # Compute median per channel\n",
    "        return np.where(np.isnan(array), median_vals, array)  # Replace NaNs\n",
    "\n",
    "    # Fix NaNs before converting to tensors\n",
    "    for i, x in enumerate(batch):\n",
    "        x['cgm_data'] = fix_nans(x['cgm_data'])\n",
    "        x['activity_data'] = fix_nans(x['activity_data'])\n",
    "\n",
    "    return {\n",
    "        'subject_ids': torch.tensor([x['subject_id'] for x in batch]),\n",
    "        'days': [x['day'] for x in batch],\n",
    "        'cgm_data': torch.stack([torch.tensor(x['cgm_data'], dtype=torch.float32) for x in batch]),\n",
    "        'activity_data': torch.stack([torch.tensor(x['activity_data'], dtype=torch.float32) for x in batch]),\n",
    "        'images': [x['images'] for x in batch],  # List of lists (variable length)\n",
    "        'nutrition': [x['nutrition'] for x in batch],  # List of lists\n",
    "        'subject_day_pairs': [x['subject_day_pair'] for x in batch]\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=custom_collate2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=custom_collate2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/llzrb2pd6hgf5s9k02bw3cq40000gn/T/ipykernel_17028/1158003765.py:342: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(os.path.join(self.data_dir, self.data_files[file_idx]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.03312733, -0.03312733, -0.03312733, ...,  0.33720096,\n",
       "        0.33720096,  0.33720096])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['activity_data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/llzrb2pd6hgf5s9k02bw3cq40000gn/T/ipykernel_17028/1158003765.py:342: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(os.path.join(self.data_dir, self.data_files[file_idx]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[3]['activity_data']\n",
    "np.isnan(train_dataset[0]['cgm_data'][0]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6221, -0.6057, -0.5854,  ...,  1.0794,  1.0635,  1.0475],\n",
      "        [-1.4413, -1.4321, -1.4227,  ...,  0.2647,  0.2417,  0.2183]],\n",
      "       dtype=torch.float64)\n",
      "tensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/llzrb2pd6hgf5s9k02bw3cq40000gn/T/ipykernel_17028/2010078360.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(os.path.join(self.data_dir, self.data_files[file_idx]))\n"
     ]
    }
   ],
   "source": [
    "for batch_dict in train_loader:\n",
    "    break\n",
    "\n",
    "\n",
    "print(batch_dict['cgm_data'][0])\n",
    "print(torch.isnan(batch_dict['cgm_data'][0]).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # Import torch.nn\n",
    "import torch.nn.functional as F \n",
    "class RMSRELoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon  # Small value to avoid division by zero\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        relative_error = (pred - target) / (target + self.epsilon)\n",
    "        squared_rel_error = relative_error ** 2\n",
    "        mean_squared_rel_error = torch.mean(squared_rel_error)\n",
    "        return torch.sqrt(mean_squared_rel_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/llzrb2pd6hgf5s9k02bw3cq40000gn/T/ipykernel_17028/1158003765.py:342: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(os.path.join(self.data_dir, self.data_files[file_idx]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_data(loader):\n",
    "    for batch in loader:\n",
    "        # Check CGM data\n",
    "        if torch.isnan(batch['cgm_data']).any():\n",
    "            print(\"NaN in CGM data!\")\n",
    "            nan_cgm_indices = torch.isnan(batch['cgm_data']).nonzero(as_tuple=True)[0]\n",
    "            for idx in nan_cgm_indices:\n",
    "                nan_value = batch['cgm_data'][idx]\n",
    "                print(f\"NaN in CGM data at index {idx.item()}: {nan_value}\")\n",
    "                break\n",
    "            return False\n",
    "        \n",
    "        # Check activity data\n",
    "        if torch.isnan(batch['activity_data']).any():\n",
    "            print(\"NaN in activity data!\")\n",
    "            nan_activity_indices = torch.isnan(batch['activity_data']).nonzero(as_tuple=True)[0]\n",
    "            for idx in nan_activity_indices:\n",
    "                nan_value = batch['activity_data'][idx]\n",
    "                print(f\"NaN in activity data at index {idx.item()}: {nan_value}\")\n",
    "                break\n",
    "            return False\n",
    "        \n",
    "        # Check nutrition values\n",
    "        for day_meals in batch['nutrition']:\n",
    "            for meal in day_meals:\n",
    "                if not isinstance(meal['calories'], (int, float)) or meal['calories'] < 0:\n",
    "                    print(f\"Invalid calorie value: {meal['calories']}\")\n",
    "                    return False\n",
    "    \n",
    "    print(\"Data validation passed!\")\n",
    "    return True\n",
    "\n",
    "# Example usage before training\n",
    "assert check_data(test_loader), \"Invalid training data detected\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import MultiheadAttention as TransformerEncoder\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Set device to MPS for Apple Silicon (M3 Mac)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the CGM and Activity Encoders\n",
    "cgm_model = TransformerEncoder(\n",
    "    n_features=180,\n",
    "    embed_dim=96,\n",
    "    num_heads=2,\n",
    "    num_classes=64,\n",
    "    dropout=0.2,\n",
    "    num_layers=3,\n",
    ").to(device)\n",
    "\n",
    "activity_model = TransformerEncoder(\n",
    "    n_features=180,\n",
    "    embed_dim=96,\n",
    "    num_heads=2,\n",
    "    num_classes=64,\n",
    "    dropout=0.2,\n",
    "    num_layers=3,\n",
    ").to(device)\n",
    "\n",
    "# Example forward pass\n",
    "x = torch.randn(32, 1, 180).to(device)\n",
    "output = cgm_model(x)\n",
    "print(output.shape)  # Should output: torch.Size([32, 64])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python MLFinalProject",
   "language": "python",
   "name": "myl6332"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
